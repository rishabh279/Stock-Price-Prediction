{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "stock_predictions_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzTx4uYyKir_"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ATjRJG-FKiwQ",
        "outputId": "ab2da681-e585-4b1d-ae35-16351888ac1d"
      },
      "source": [
        "stock_data = pd.read_csv('NSE-Tata-Global-Beverages-Limited.csv', parse_dates=['Date'])\n",
        "stock_data.sort_values(by='Date', ascending=True, inplace=True)\n",
        "stock_data = stock_data.reset_index()\n",
        "stock_data.rename(columns={'Turnover (Lacs)': 'Turnover'}, inplace=True)\n",
        "stock_data['Price'] = (stock_data['Turnover'] / stock_data['Total Trade Quantity']) * 100000\n",
        "stock_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Last</th>\n",
              "      <th>Close</th>\n",
              "      <th>Total Trade Quantity</th>\n",
              "      <th>Turnover</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1234</td>\n",
              "      <td>2013-10-08</td>\n",
              "      <td>157.00</td>\n",
              "      <td>157.80</td>\n",
              "      <td>155.20</td>\n",
              "      <td>155.8</td>\n",
              "      <td>155.80</td>\n",
              "      <td>1720413.0</td>\n",
              "      <td>2688.94</td>\n",
              "      <td>156.296192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1233</td>\n",
              "      <td>2013-10-09</td>\n",
              "      <td>155.70</td>\n",
              "      <td>158.20</td>\n",
              "      <td>154.15</td>\n",
              "      <td>155.3</td>\n",
              "      <td>155.55</td>\n",
              "      <td>2049580.0</td>\n",
              "      <td>3204.49</td>\n",
              "      <td>156.348618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1232</td>\n",
              "      <td>2013-10-10</td>\n",
              "      <td>156.00</td>\n",
              "      <td>160.80</td>\n",
              "      <td>155.85</td>\n",
              "      <td>160.3</td>\n",
              "      <td>160.15</td>\n",
              "      <td>3124853.0</td>\n",
              "      <td>4978.80</td>\n",
              "      <td>159.329095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1231</td>\n",
              "      <td>2013-10-11</td>\n",
              "      <td>161.15</td>\n",
              "      <td>163.45</td>\n",
              "      <td>159.00</td>\n",
              "      <td>159.8</td>\n",
              "      <td>160.05</td>\n",
              "      <td>1880046.0</td>\n",
              "      <td>3030.76</td>\n",
              "      <td>161.206694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1230</td>\n",
              "      <td>2013-10-14</td>\n",
              "      <td>160.85</td>\n",
              "      <td>161.45</td>\n",
              "      <td>157.70</td>\n",
              "      <td>159.3</td>\n",
              "      <td>159.45</td>\n",
              "      <td>1281419.0</td>\n",
              "      <td>2039.09</td>\n",
              "      <td>159.127498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index       Date    Open  ...  Total Trade Quantity  Turnover       Price\n",
              "0   1234 2013-10-08  157.00  ...             1720413.0   2688.94  156.296192\n",
              "1   1233 2013-10-09  155.70  ...             2049580.0   3204.49  156.348618\n",
              "2   1232 2013-10-10  156.00  ...             3124853.0   4978.80  159.329095\n",
              "3   1231 2013-10-11  161.15  ...             1880046.0   3030.76  161.206694\n",
              "4   1230 2013-10-14  160.85  ...             1281419.0   2039.09  159.127498\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6ASfUdNKi2K"
      },
      "source": [
        "def sliding_windows(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        _x = data[i:(i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "seq_length = 5\n",
        "price_data = stock_data['Price'].to_frame()\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "price_data = sc.fit_transform(price_data)\n",
        "x, y = sliding_windows(price_data, seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds4uWhgsKi5b",
        "outputId": "7655b015-f2c5-4b35-fb0e-8c23a265b50f"
      },
      "source": [
        "class LSTMForecast(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMForecast, self).__init__()\n",
        "        self.input_size = input_size \n",
        "        self.hidden_size = hidden_size \n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
        "                            num_layers=num_layers, dropout=0.2)\n",
        "        self.linear = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        output, (h, c) = self.lstm(input)\n",
        "        h = h.to(device)\n",
        "        out = self.linear(h)\n",
        "        return out\n",
        "\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.0001\n",
        "batch_size = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_size = 1\n",
        "num_layers = 1\n",
        "hidden_size = 256\n",
        "\n",
        "x_train, x_test, y_train, y_test = x[:-5], x[-5:], y[:-5], y[-5:]\n",
        "x_train = torch.from_numpy(x_train).type(torch.FloatTensor) \n",
        "x_test = torch.from_numpy(x_test).type(torch.FloatTensor) \n",
        "y_train = torch.from_numpy(y_train).type(torch.FloatTensor) \n",
        "y_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
        "\n",
        "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "test = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "model = LSTMForecast(input_size, hidden_size, num_layers, 1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.MSELoss(reduction = 'mean')\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        inp_data = batch[0].to(device)\n",
        "        inp_data = inp_data.permute(1,0,2)\n",
        "        target = batch[1].to(device)\n",
        "        model = model.to(device)\n",
        "        output = model(inp_data)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = criterion(output.squeeze(0), target)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch} / {num_epochs} done..')\n",
        "    print(f'Training Loss {loss}')\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_loader):\n",
        "        inp_data = batch[0].to(device)\n",
        "        inp_data = inp_data.permute(1,0,2)\n",
        "        target = batch[1].to(device)\n",
        "        output = model(inp_data)\n",
        "        p = torch.from_numpy(sc.inverse_transform(output.cpu().data.numpy().squeeze(0)))\n",
        "        o = torch.from_numpy(sc.inverse_transform(target.cpu().data.numpy()))\n",
        "        loss = criterion(p, o)\n",
        "        print(loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 / 1000 done..\n",
            "Training Loss 0.255391389131546\n",
            "Epoch 1 / 1000 done..\n",
            "Training Loss 0.12345286458730698\n",
            "Epoch 2 / 1000 done..\n",
            "Training Loss 0.08682452142238617\n",
            "Epoch 3 / 1000 done..\n",
            "Training Loss 0.07680217921733856\n",
            "Epoch 4 / 1000 done..\n",
            "Training Loss 0.06690093874931335\n",
            "Epoch 5 / 1000 done..\n",
            "Training Loss 0.05729573220014572\n",
            "Epoch 6 / 1000 done..\n",
            "Training Loss 0.04735305532813072\n",
            "Epoch 7 / 1000 done..\n",
            "Training Loss 0.036592721939086914\n",
            "Epoch 8 / 1000 done..\n",
            "Training Loss 0.024913305416703224\n",
            "Epoch 9 / 1000 done..\n",
            "Training Loss 0.013292115181684494\n",
            "Epoch 10 / 1000 done..\n",
            "Training Loss 0.005001794546842575\n",
            "Epoch 11 / 1000 done..\n",
            "Training Loss 0.0031629616860300303\n",
            "Epoch 12 / 1000 done..\n",
            "Training Loss 0.0040291729383170605\n",
            "Epoch 13 / 1000 done..\n",
            "Training Loss 0.0030566551722586155\n",
            "Epoch 14 / 1000 done..\n",
            "Training Loss 0.002764800563454628\n",
            "Epoch 15 / 1000 done..\n",
            "Training Loss 0.0029321545735001564\n",
            "Epoch 16 / 1000 done..\n",
            "Training Loss 0.0029030211735516787\n",
            "Epoch 17 / 1000 done..\n",
            "Training Loss 0.0027585779316723347\n",
            "Epoch 18 / 1000 done..\n",
            "Training Loss 0.002685127779841423\n",
            "Epoch 19 / 1000 done..\n",
            "Training Loss 0.002632634714245796\n",
            "Epoch 20 / 1000 done..\n",
            "Training Loss 0.0025556273758411407\n",
            "Epoch 21 / 1000 done..\n",
            "Training Loss 0.0024731906596571207\n",
            "Epoch 22 / 1000 done..\n",
            "Training Loss 0.0023958112578839064\n",
            "Epoch 23 / 1000 done..\n",
            "Training Loss 0.0023175489623099566\n",
            "Epoch 24 / 1000 done..\n",
            "Training Loss 0.0022366850171238184\n",
            "Epoch 25 / 1000 done..\n",
            "Training Loss 0.0021550857927650213\n",
            "Epoch 26 / 1000 done..\n",
            "Training Loss 0.002073250012472272\n",
            "Epoch 27 / 1000 done..\n",
            "Training Loss 0.0019910132978111506\n",
            "Epoch 28 / 1000 done..\n",
            "Training Loss 0.0019086163956671953\n",
            "Epoch 29 / 1000 done..\n",
            "Training Loss 0.001826382358558476\n",
            "Epoch 30 / 1000 done..\n",
            "Training Loss 0.0017445735866203904\n",
            "Epoch 31 / 1000 done..\n",
            "Training Loss 0.0016634623752906919\n",
            "Epoch 32 / 1000 done..\n",
            "Training Loss 0.001583350938744843\n",
            "Epoch 33 / 1000 done..\n",
            "Training Loss 0.001504535903222859\n",
            "Epoch 34 / 1000 done..\n",
            "Training Loss 0.0014273262349888682\n",
            "Epoch 35 / 1000 done..\n",
            "Training Loss 0.0013520446373149753\n",
            "Epoch 36 / 1000 done..\n",
            "Training Loss 0.0012789982138201594\n",
            "Epoch 37 / 1000 done..\n",
            "Training Loss 0.0012084988411515951\n",
            "Epoch 38 / 1000 done..\n",
            "Training Loss 0.0011408468708395958\n",
            "Epoch 39 / 1000 done..\n",
            "Training Loss 0.001076316460967064\n",
            "Epoch 40 / 1000 done..\n",
            "Training Loss 0.0010151673341169953\n",
            "Epoch 41 / 1000 done..\n",
            "Training Loss 0.0009576287120580673\n",
            "Epoch 42 / 1000 done..\n",
            "Training Loss 0.0009038839489221573\n",
            "Epoch 43 / 1000 done..\n",
            "Training Loss 0.0008540835697203875\n",
            "Epoch 44 / 1000 done..\n",
            "Training Loss 0.0008083106367848814\n",
            "Epoch 45 / 1000 done..\n",
            "Training Loss 0.0007666186429560184\n",
            "Epoch 46 / 1000 done..\n",
            "Training Loss 0.0007289904751814902\n",
            "Epoch 47 / 1000 done..\n",
            "Training Loss 0.0006953622796572745\n",
            "Epoch 48 / 1000 done..\n",
            "Training Loss 0.0006656141486018896\n",
            "Epoch 49 / 1000 done..\n",
            "Training Loss 0.000639585661701858\n",
            "Epoch 50 / 1000 done..\n",
            "Training Loss 0.0006170621491037309\n",
            "Epoch 51 / 1000 done..\n",
            "Training Loss 0.0005978032131679356\n",
            "Epoch 52 / 1000 done..\n",
            "Training Loss 0.0005815346958115697\n",
            "Epoch 53 / 1000 done..\n",
            "Training Loss 0.0005679749883711338\n",
            "Epoch 54 / 1000 done..\n",
            "Training Loss 0.0005568229244090617\n",
            "Epoch 55 / 1000 done..\n",
            "Training Loss 0.0005477841477841139\n",
            "Epoch 56 / 1000 done..\n",
            "Training Loss 0.0005405681440606713\n",
            "Epoch 57 / 1000 done..\n",
            "Training Loss 0.0005349015118554235\n",
            "Epoch 58 / 1000 done..\n",
            "Training Loss 0.0005305306985974312\n",
            "Epoch 59 / 1000 done..\n",
            "Training Loss 0.0005272245034575462\n",
            "Epoch 60 / 1000 done..\n",
            "Training Loss 0.0005247773369774222\n",
            "Epoch 61 / 1000 done..\n",
            "Training Loss 0.0005230078240856528\n",
            "Epoch 62 / 1000 done..\n",
            "Training Loss 0.0005217663128860295\n",
            "Epoch 63 / 1000 done..\n",
            "Training Loss 0.000520923756994307\n",
            "Epoch 64 / 1000 done..\n",
            "Training Loss 0.0005203748005442321\n",
            "Epoch 65 / 1000 done..\n",
            "Training Loss 0.0005200368468649685\n",
            "Epoch 66 / 1000 done..\n",
            "Training Loss 0.0005198410362936556\n",
            "Epoch 67 / 1000 done..\n",
            "Training Loss 0.0005197381251491606\n",
            "Epoch 68 / 1000 done..\n",
            "Training Loss 0.0005196909187361598\n",
            "Epoch 69 / 1000 done..\n",
            "Training Loss 0.0005196667043492198\n",
            "Epoch 70 / 1000 done..\n",
            "Training Loss 0.0005196487181819975\n",
            "Epoch 71 / 1000 done..\n",
            "Training Loss 0.0005196212441660464\n",
            "Epoch 72 / 1000 done..\n",
            "Training Loss 0.0005195763078518212\n",
            "Epoch 73 / 1000 done..\n",
            "Training Loss 0.0005195093108341098\n",
            "Epoch 74 / 1000 done..\n",
            "Training Loss 0.000519416993483901\n",
            "Epoch 75 / 1000 done..\n",
            "Training Loss 0.0005192961543798447\n",
            "Epoch 76 / 1000 done..\n",
            "Training Loss 0.0005191500531509519\n",
            "Epoch 77 / 1000 done..\n",
            "Training Loss 0.0005189763614907861\n",
            "Epoch 78 / 1000 done..\n",
            "Training Loss 0.0005187804345041513\n",
            "Epoch 79 / 1000 done..\n",
            "Training Loss 0.0005185608752071857\n",
            "Epoch 80 / 1000 done..\n",
            "Training Loss 0.0005183188477531075\n",
            "Epoch 81 / 1000 done..\n",
            "Training Loss 0.000518058193847537\n",
            "Epoch 82 / 1000 done..\n",
            "Training Loss 0.0005177800776436925\n",
            "Epoch 83 / 1000 done..\n",
            "Training Loss 0.0005174871766939759\n",
            "Epoch 84 / 1000 done..\n",
            "Training Loss 0.0005171779193915427\n",
            "Epoch 85 / 1000 done..\n",
            "Training Loss 0.0005168557399883866\n",
            "Epoch 86 / 1000 done..\n",
            "Training Loss 0.0005165227339603007\n",
            "Epoch 87 / 1000 done..\n",
            "Training Loss 0.0005161765730008483\n",
            "Epoch 88 / 1000 done..\n",
            "Training Loss 0.0005158231360837817\n",
            "Epoch 89 / 1000 done..\n",
            "Training Loss 0.0005154591635800898\n",
            "Epoch 90 / 1000 done..\n",
            "Training Loss 0.0005150865763425827\n",
            "Epoch 91 / 1000 done..\n",
            "Training Loss 0.0005147064803168178\n",
            "Epoch 92 / 1000 done..\n",
            "Training Loss 0.0005143202724866569\n",
            "Epoch 93 / 1000 done..\n",
            "Training Loss 0.0005139262648299336\n",
            "Epoch 94 / 1000 done..\n",
            "Training Loss 0.0005135271931067109\n",
            "Epoch 95 / 1000 done..\n",
            "Training Loss 0.0005131227662786841\n",
            "Epoch 96 / 1000 done..\n",
            "Training Loss 0.0005127115873619914\n",
            "Epoch 97 / 1000 done..\n",
            "Training Loss 0.0005122966249473393\n",
            "Epoch 98 / 1000 done..\n",
            "Training Loss 0.0005118753761053085\n",
            "Epoch 99 / 1000 done..\n",
            "Training Loss 0.0005114524392411113\n",
            "Epoch 100 / 1000 done..\n",
            "Training Loss 0.00051102414727211\n",
            "Epoch 101 / 1000 done..\n",
            "Training Loss 0.0005105925956740975\n",
            "Epoch 102 / 1000 done..\n",
            "Training Loss 0.0005101575516164303\n",
            "Epoch 103 / 1000 done..\n",
            "Training Loss 0.0005097176763229072\n",
            "Epoch 104 / 1000 done..\n",
            "Training Loss 0.0005092776846140623\n",
            "Epoch 105 / 1000 done..\n",
            "Training Loss 0.0005088313482701778\n",
            "Epoch 106 / 1000 done..\n",
            "Training Loss 0.0005083838477730751\n",
            "Epoch 107 / 1000 done..\n",
            "Training Loss 0.000507933902554214\n",
            "Epoch 108 / 1000 done..\n",
            "Training Loss 0.0005074812797829509\n",
            "Epoch 109 / 1000 done..\n",
            "Training Loss 0.0005070273182354867\n",
            "Epoch 110 / 1000 done..\n",
            "Training Loss 0.0005065689911134541\n",
            "Epoch 111 / 1000 done..\n",
            "Training Loss 0.0005061080446466804\n",
            "Epoch 112 / 1000 done..\n",
            "Training Loss 0.0005056479712948203\n",
            "Epoch 113 / 1000 done..\n",
            "Training Loss 0.0005051832413300872\n",
            "Epoch 114 / 1000 done..\n",
            "Training Loss 0.0005047187441959977\n",
            "Epoch 115 / 1000 done..\n",
            "Training Loss 0.0005042509874328971\n",
            "Epoch 116 / 1000 done..\n",
            "Training Loss 0.0005037827650085092\n",
            "Epoch 117 / 1000 done..\n",
            "Training Loss 0.0005033102352172136\n",
            "Epoch 118 / 1000 done..\n",
            "Training Loss 0.0005028388695791364\n",
            "Epoch 119 / 1000 done..\n",
            "Training Loss 0.0005023645353503525\n",
            "Epoch 120 / 1000 done..\n",
            "Training Loss 0.000501887989230454\n",
            "Epoch 121 / 1000 done..\n",
            "Training Loss 0.00050141173414886\n",
            "Epoch 122 / 1000 done..\n",
            "Training Loss 0.0005009341984987259\n",
            "Epoch 123 / 1000 done..\n",
            "Training Loss 0.0005004548002034426\n",
            "Epoch 124 / 1000 done..\n",
            "Training Loss 0.0004999724333174527\n",
            "Epoch 125 / 1000 done..\n",
            "Training Loss 0.0004994902992621064\n",
            "Epoch 126 / 1000 done..\n",
            "Training Loss 0.0004990082234144211\n",
            "Epoch 127 / 1000 done..\n",
            "Training Loss 0.0004985224222764373\n",
            "Epoch 128 / 1000 done..\n",
            "Training Loss 0.000498036970384419\n",
            "Epoch 129 / 1000 done..\n",
            "Training Loss 0.0004975509946234524\n",
            "Epoch 130 / 1000 done..\n",
            "Training Loss 0.0004970642039552331\n",
            "Epoch 131 / 1000 done..\n",
            "Training Loss 0.0004965740954503417\n",
            "Epoch 132 / 1000 done..\n",
            "Training Loss 0.0004960843361914158\n",
            "Epoch 133 / 1000 done..\n",
            "Training Loss 0.0004955937038175762\n",
            "Epoch 134 / 1000 done..\n",
            "Training Loss 0.0004951029550284147\n",
            "Epoch 135 / 1000 done..\n",
            "Training Loss 0.000494610343594104\n",
            "Epoch 136 / 1000 done..\n",
            "Training Loss 0.0004941171500831842\n",
            "Epoch 137 / 1000 done..\n",
            "Training Loss 0.0004936241311952472\n",
            "Epoch 138 / 1000 done..\n",
            "Training Loss 0.0004931294242851436\n",
            "Epoch 139 / 1000 done..\n",
            "Training Loss 0.0004926322144456208\n",
            "Epoch 140 / 1000 done..\n",
            "Training Loss 0.000492137623950839\n",
            "Epoch 141 / 1000 done..\n",
            "Training Loss 0.0004916402976959944\n",
            "Epoch 142 / 1000 done..\n",
            "Training Loss 0.0004911436117254198\n",
            "Epoch 143 / 1000 done..\n",
            "Training Loss 0.0004906441317871213\n",
            "Epoch 144 / 1000 done..\n",
            "Training Loss 0.0004901441279798746\n",
            "Epoch 145 / 1000 done..\n",
            "Training Loss 0.0004896447062492371\n",
            "Epoch 146 / 1000 done..\n",
            "Training Loss 0.0004891444696113467\n",
            "Epoch 147 / 1000 done..\n",
            "Training Loss 0.0004886440001428127\n",
            "Epoch 148 / 1000 done..\n",
            "Training Loss 0.00048814123147167265\n",
            "Epoch 149 / 1000 done..\n",
            "Training Loss 0.0004876398015767336\n",
            "Epoch 150 / 1000 done..\n",
            "Training Loss 0.00048713586875237525\n",
            "Epoch 151 / 1000 done..\n",
            "Training Loss 0.0004866333329118788\n",
            "Epoch 152 / 1000 done..\n",
            "Training Loss 0.00048612861428409815\n",
            "Epoch 153 / 1000 done..\n",
            "Training Loss 0.0004856257000938058\n",
            "Epoch 154 / 1000 done..\n",
            "Training Loss 0.0004851205158047378\n",
            "Epoch 155 / 1000 done..\n",
            "Training Loss 0.00048461416736245155\n",
            "Epoch 156 / 1000 done..\n",
            "Training Loss 0.0004841082263737917\n",
            "Epoch 157 / 1000 done..\n",
            "Training Loss 0.00048360141227021813\n",
            "Epoch 158 / 1000 done..\n",
            "Training Loss 0.0004830947145819664\n",
            "Epoch 159 / 1000 done..\n",
            "Training Loss 0.0004825885407626629\n",
            "Epoch 160 / 1000 done..\n",
            "Training Loss 0.0004820824833586812\n",
            "Epoch 161 / 1000 done..\n",
            "Training Loss 0.00048157383571378887\n",
            "Epoch 162 / 1000 done..\n",
            "Training Loss 0.00048106457688845694\n",
            "Epoch 163 / 1000 done..\n",
            "Training Loss 0.00048055651132017374\n",
            "Epoch 164 / 1000 done..\n",
            "Training Loss 0.0004800477472599596\n",
            "Epoch 165 / 1000 done..\n",
            "Training Loss 0.00047953834291547537\n",
            "Epoch 166 / 1000 done..\n",
            "Training Loss 0.0004790281818713993\n",
            "Epoch 167 / 1000 done..\n",
            "Training Loss 0.0004785193596035242\n",
            "Epoch 168 / 1000 done..\n",
            "Training Loss 0.0004780097515322268\n",
            "Epoch 169 / 1000 done..\n",
            "Training Loss 0.0004775003471877426\n",
            "Epoch 170 / 1000 done..\n",
            "Training Loss 0.00047699036076664925\n",
            "Epoch 171 / 1000 done..\n",
            "Training Loss 0.00047647973406128585\n",
            "Epoch 172 / 1000 done..\n",
            "Training Loss 0.00047596925287507474\n",
            "Epoch 173 / 1000 done..\n",
            "Training Loss 0.0004754567053169012\n",
            "Epoch 174 / 1000 done..\n",
            "Training Loss 0.00047494564205408096\n",
            "Epoch 175 / 1000 done..\n",
            "Training Loss 0.0004744352772831917\n",
            "Epoch 176 / 1000 done..\n",
            "Training Loss 0.0004739247669931501\n",
            "Epoch 177 / 1000 done..\n",
            "Training Loss 0.00047341303434222937\n",
            "Epoch 178 / 1000 done..\n",
            "Training Loss 0.00047290202928707004\n",
            "Epoch 179 / 1000 done..\n",
            "Training Loss 0.00047239047125913203\n",
            "Epoch 180 / 1000 done..\n",
            "Training Loss 0.0004718791460618377\n",
            "Epoch 181 / 1000 done..\n",
            "Training Loss 0.0004713669768534601\n",
            "Epoch 182 / 1000 done..\n",
            "Training Loss 0.0004708553315140307\n",
            "Epoch 183 / 1000 done..\n",
            "Training Loss 0.0004703437152784318\n",
            "Epoch 184 / 1000 done..\n",
            "Training Loss 0.0004698339325841516\n",
            "Epoch 185 / 1000 done..\n",
            "Training Loss 0.0004693217924796045\n",
            "Epoch 186 / 1000 done..\n",
            "Training Loss 0.0004688100307248533\n",
            "Epoch 187 / 1000 done..\n",
            "Training Loss 0.0004682989092543721\n",
            "Epoch 188 / 1000 done..\n",
            "Training Loss 0.00046778845717199147\n",
            "Epoch 189 / 1000 done..\n",
            "Training Loss 0.00046727684093639255\n",
            "Epoch 190 / 1000 done..\n",
            "Training Loss 0.0004667640896514058\n",
            "Epoch 191 / 1000 done..\n",
            "Training Loss 0.00046625419054180384\n",
            "Epoch 192 / 1000 done..\n",
            "Training Loss 0.0004657442623283714\n",
            "Epoch 193 / 1000 done..\n",
            "Training Loss 0.00046523159835487604\n",
            "Epoch 194 / 1000 done..\n",
            "Training Loss 0.000464721757452935\n",
            "Epoch 195 / 1000 done..\n",
            "Training Loss 0.00046421156730502844\n",
            "Epoch 196 / 1000 done..\n",
            "Training Loss 0.00046370108611881733\n",
            "Epoch 197 / 1000 done..\n",
            "Training Loss 0.00046319118700921535\n",
            "Epoch 198 / 1000 done..\n",
            "Training Loss 0.0004626799200195819\n",
            "Epoch 199 / 1000 done..\n",
            "Training Loss 0.00046217013732530177\n",
            "Epoch 200 / 1000 done..\n",
            "Training Loss 0.000461660441942513\n",
            "Epoch 201 / 1000 done..\n",
            "Training Loss 0.00046115167788229883\n",
            "Epoch 202 / 1000 done..\n",
            "Training Loss 0.0004606412840075791\n",
            "Epoch 203 / 1000 done..\n",
            "Training Loss 0.00046013371320441365\n",
            "Epoch 204 / 1000 done..\n",
            "Training Loss 0.00045962462900206447\n",
            "Epoch 205 / 1000 done..\n",
            "Training Loss 0.0004591160686686635\n",
            "Epoch 206 / 1000 done..\n",
            "Training Loss 0.00045860750833526254\n",
            "Epoch 207 / 1000 done..\n",
            "Training Loss 0.000458098656963557\n",
            "Epoch 208 / 1000 done..\n",
            "Training Loss 0.0004575924249365926\n",
            "Epoch 209 / 1000 done..\n",
            "Training Loss 0.00045708264224231243\n",
            "Epoch 210 / 1000 done..\n",
            "Training Loss 0.0004565772251226008\n",
            "Epoch 211 / 1000 done..\n",
            "Training Loss 0.00045606913045048714\n",
            "Epoch 212 / 1000 done..\n",
            "Training Loss 0.00045556185068562627\n",
            "Epoch 213 / 1000 done..\n",
            "Training Loss 0.0004550546873360872\n",
            "Epoch 214 / 1000 done..\n",
            "Training Loss 0.00045454775681719184\n",
            "Epoch 215 / 1000 done..\n",
            "Training Loss 0.0004540422232821584\n",
            "Epoch 216 / 1000 done..\n",
            "Training Loss 0.0004535357002168894\n",
            "Epoch 217 / 1000 done..\n",
            "Training Loss 0.00045302839134819806\n",
            "Epoch 218 / 1000 done..\n",
            "Training Loss 0.00045252390555106103\n",
            "Epoch 219 / 1000 done..\n",
            "Training Loss 0.0004520182847045362\n",
            "Epoch 220 / 1000 done..\n",
            "Training Loss 0.00045151315862312913\n",
            "Epoch 221 / 1000 done..\n",
            "Training Loss 0.0004510080034378916\n",
            "Epoch 222 / 1000 done..\n",
            "Training Loss 0.0004505033721216023\n",
            "Epoch 223 / 1000 done..\n",
            "Training Loss 0.0004499977803789079\n",
            "Epoch 224 / 1000 done..\n",
            "Training Loss 0.00044949393486604095\n",
            "Epoch 225 / 1000 done..\n",
            "Training Loss 0.0004489881976041943\n",
            "Epoch 226 / 1000 done..\n",
            "Training Loss 0.00044848473044112325\n",
            "Epoch 227 / 1000 done..\n",
            "Training Loss 0.00044798001181334257\n",
            "Epoch 228 / 1000 done..\n",
            "Training Loss 0.0004474760207813233\n",
            "Epoch 229 / 1000 done..\n",
            "Training Loss 0.0004469726700335741\n",
            "Epoch 230 / 1000 done..\n",
            "Training Loss 0.0004464681842364371\n",
            "Epoch 231 / 1000 done..\n",
            "Training Loss 0.0004459638730622828\n",
            "Epoch 232 / 1000 done..\n",
            "Training Loss 0.0004454604641068727\n",
            "Epoch 233 / 1000 done..\n",
            "Training Loss 0.00044495699694380164\n",
            "Epoch 234 / 1000 done..\n",
            "Training Loss 0.00044445382081903517\n",
            "Epoch 235 / 1000 done..\n",
            "Training Loss 0.0004439491021912545\n",
            "Epoch 236 / 1000 done..\n",
            "Training Loss 0.00044344551861286163\n",
            "Epoch 237 / 1000 done..\n",
            "Training Loss 0.0004429426626302302\n",
            "Epoch 238 / 1000 done..\n",
            "Training Loss 0.0004424392245709896\n",
            "Epoch 239 / 1000 done..\n",
            "Training Loss 0.00044193564099259675\n",
            "Epoch 240 / 1000 done..\n",
            "Training Loss 0.00044143182458356023\n",
            "Epoch 241 / 1000 done..\n",
            "Training Loss 0.0004409270186442882\n",
            "Epoch 242 / 1000 done..\n",
            "Training Loss 0.0004404241335578263\n",
            "Epoch 243 / 1000 done..\n",
            "Training Loss 0.0004399231111165136\n",
            "Epoch 244 / 1000 done..\n",
            "Training Loss 0.00043941789772361517\n",
            "Epoch 245 / 1000 done..\n",
            "Training Loss 0.00043891489622183144\n",
            "Epoch 246 / 1000 done..\n",
            "Training Loss 0.0004384104395285249\n",
            "Epoch 247 / 1000 done..\n",
            "Training Loss 0.0004379071178846061\n",
            "Epoch 248 / 1000 done..\n",
            "Training Loss 0.000437404029071331\n",
            "Epoch 249 / 1000 done..\n",
            "Training Loss 0.00043690032907761633\n",
            "Epoch 250 / 1000 done..\n",
            "Training Loss 0.00043639715295284986\n",
            "Epoch 251 / 1000 done..\n",
            "Training Loss 0.00043589185224846005\n",
            "Epoch 252 / 1000 done..\n",
            "Training Loss 0.000435388064943254\n",
            "Epoch 253 / 1000 done..\n",
            "Training Loss 0.00043488340452313423\n",
            "Epoch 254 / 1000 done..\n",
            "Training Loss 0.0004343811306171119\n",
            "Epoch 255 / 1000 done..\n",
            "Training Loss 0.0004338744329288602\n",
            "Epoch 256 / 1000 done..\n",
            "Training Loss 0.00043337169336155057\n",
            "Epoch 257 / 1000 done..\n",
            "Training Loss 0.000432866218034178\n",
            "Epoch 258 / 1000 done..\n",
            "Training Loss 0.00043236350757069886\n",
            "Epoch 259 / 1000 done..\n",
            "Training Loss 0.00043185881804674864\n",
            "Epoch 260 / 1000 done..\n",
            "Training Loss 0.00043135276064276695\n",
            "Epoch 261 / 1000 done..\n",
            "Training Loss 0.00043084778008051217\n",
            "Epoch 262 / 1000 done..\n",
            "Training Loss 0.0004303425666876137\n",
            "Epoch 263 / 1000 done..\n",
            "Training Loss 0.0004298372659832239\n",
            "Epoch 264 / 1000 done..\n",
            "Training Loss 0.0004293309757485986\n",
            "Epoch 265 / 1000 done..\n",
            "Training Loss 0.00042882762500084937\n",
            "Epoch 266 / 1000 done..\n",
            "Training Loss 0.0004283217713236809\n",
            "Epoch 267 / 1000 done..\n",
            "Training Loss 0.0004278153064660728\n",
            "Epoch 268 / 1000 done..\n",
            "Training Loss 0.0004273096565157175\n",
            "Epoch 269 / 1000 done..\n",
            "Training Loss 0.0004268034244887531\n",
            "Epoch 270 / 1000 done..\n",
            "Training Loss 0.0004262958245817572\n",
            "Epoch 271 / 1000 done..\n",
            "Training Loss 0.00042578898137435317\n",
            "Epoch 272 / 1000 done..\n",
            "Training Loss 0.0004252821090631187\n",
            "Epoch 273 / 1000 done..\n",
            "Training Loss 0.00042477279203012586\n",
            "Epoch 274 / 1000 done..\n",
            "Training Loss 0.00042426702566444874\n",
            "Epoch 275 / 1000 done..\n",
            "Training Loss 0.0004237573593854904\n",
            "Epoch 276 / 1000 done..\n",
            "Training Loss 0.0004232503124512732\n",
            "Epoch 277 / 1000 done..\n",
            "Training Loss 0.00042274000588804483\n",
            "Epoch 278 / 1000 done..\n",
            "Training Loss 0.0004222314746584743\n",
            "Epoch 279 / 1000 done..\n",
            "Training Loss 0.0004217219538986683\n",
            "Epoch 280 / 1000 done..\n",
            "Training Loss 0.0004212118801660836\n",
            "Epoch 281 / 1000 done..\n",
            "Training Loss 0.0004207012243568897\n",
            "Epoch 282 / 1000 done..\n",
            "Training Loss 0.00042019234388135374\n",
            "Epoch 283 / 1000 done..\n",
            "Training Loss 0.0004196798545308411\n",
            "Epoch 284 / 1000 done..\n",
            "Training Loss 0.0004191689076833427\n",
            "Epoch 285 / 1000 done..\n",
            "Training Loss 0.0004186558653600514\n",
            "Epoch 286 / 1000 done..\n",
            "Training Loss 0.00041814325959421694\n",
            "Epoch 287 / 1000 done..\n",
            "Training Loss 0.0004176297225058079\n",
            "Epoch 288 / 1000 done..\n",
            "Training Loss 0.00041711475932970643\n",
            "Epoch 289 / 1000 done..\n",
            "Training Loss 0.0004166007856838405\n",
            "Epoch 290 / 1000 done..\n",
            "Training Loss 0.0004160867538303137\n",
            "Epoch 291 / 1000 done..\n",
            "Training Loss 0.000415570946643129\n",
            "Epoch 292 / 1000 done..\n",
            "Training Loss 0.00041505618719384074\n",
            "Epoch 293 / 1000 done..\n",
            "Training Loss 0.0004145392158534378\n",
            "Epoch 294 / 1000 done..\n",
            "Training Loss 0.00041402369970455766\n",
            "Epoch 295 / 1000 done..\n",
            "Training Loss 0.00041350547689944506\n",
            "Epoch 296 / 1000 done..\n",
            "Training Loss 0.00041298818541690707\n",
            "Epoch 297 / 1000 done..\n",
            "Training Loss 0.0004124692641198635\n",
            "Epoch 298 / 1000 done..\n",
            "Training Loss 0.0004119521181564778\n",
            "Epoch 299 / 1000 done..\n",
            "Training Loss 0.000411432672990486\n",
            "Epoch 300 / 1000 done..\n",
            "Training Loss 0.00041091267485171556\n",
            "Epoch 301 / 1000 done..\n",
            "Training Loss 0.00041039378265850246\n",
            "Epoch 302 / 1000 done..\n",
            "Training Loss 0.0004098729114048183\n",
            "Epoch 303 / 1000 done..\n",
            "Training Loss 0.0004093539319001138\n",
            "Epoch 304 / 1000 done..\n",
            "Training Loss 0.0004088334389962256\n",
            "Epoch 305 / 1000 done..\n",
            "Training Loss 0.00040831248043105006\n",
            "Epoch 306 / 1000 done..\n",
            "Training Loss 0.0004077922203578055\n",
            "Epoch 307 / 1000 done..\n",
            "Training Loss 0.00040727161103859544\n",
            "Epoch 308 / 1000 done..\n",
            "Training Loss 0.00040675175841897726\n",
            "Epoch 309 / 1000 done..\n",
            "Training Loss 0.0004062306834384799\n",
            "Epoch 310 / 1000 done..\n",
            "Training Loss 0.0004057094338349998\n",
            "Epoch 311 / 1000 done..\n",
            "Training Loss 0.00040519036701880395\n",
            "Epoch 312 / 1000 done..\n",
            "Training Loss 0.0004046692920383066\n",
            "Epoch 313 / 1000 done..\n",
            "Training Loss 0.0004041493230033666\n",
            "Epoch 314 / 1000 done..\n",
            "Training Loss 0.00040362958679907024\n",
            "Epoch 315 / 1000 done..\n",
            "Training Loss 0.0004031095886602998\n",
            "Epoch 316 / 1000 done..\n",
            "Training Loss 0.0004025906673632562\n",
            "Epoch 317 / 1000 done..\n",
            "Training Loss 0.0004020722699351609\n",
            "Epoch 318 / 1000 done..\n",
            "Training Loss 0.0004015534359496087\n",
            "Epoch 319 / 1000 done..\n",
            "Training Loss 0.00040103617357090116\n",
            "Epoch 320 / 1000 done..\n",
            "Training Loss 0.0004005188529845327\n",
            "Epoch 321 / 1000 done..\n",
            "Training Loss 0.00040000188164412975\n",
            "Epoch 322 / 1000 done..\n",
            "Training Loss 0.00039948488119989634\n",
            "Epoch 323 / 1000 done..\n",
            "Training Loss 0.0003989694523625076\n",
            "Epoch 324 / 1000 done..\n",
            "Training Loss 0.00039845320861786604\n",
            "Epoch 325 / 1000 done..\n",
            "Training Loss 0.00039794063195586205\n",
            "Epoch 326 / 1000 done..\n",
            "Training Loss 0.0003974259307142347\n",
            "Epoch 327 / 1000 done..\n",
            "Training Loss 0.0003969125682488084\n",
            "Epoch 328 / 1000 done..\n",
            "Training Loss 0.00039639888564124703\n",
            "Epoch 329 / 1000 done..\n",
            "Training Loss 0.0003958878805860877\n",
            "Epoch 330 / 1000 done..\n",
            "Training Loss 0.0003953695413656533\n",
            "Epoch 331 / 1000 done..\n",
            "Training Loss 0.0003948483499698341\n",
            "Epoch 332 / 1000 done..\n",
            "Training Loss 0.0003942639450542629\n",
            "Epoch 333 / 1000 done..\n",
            "Training Loss 0.0003942172625102103\n",
            "Epoch 334 / 1000 done..\n",
            "Training Loss 0.00039486493915319443\n",
            "Epoch 335 / 1000 done..\n",
            "Training Loss 0.00039329472929239273\n",
            "Epoch 336 / 1000 done..\n",
            "Training Loss 0.0003925940836779773\n",
            "Epoch 337 / 1000 done..\n",
            "Training Loss 0.00039292799192480743\n",
            "Epoch 338 / 1000 done..\n",
            "Training Loss 0.0003922145697288215\n",
            "Epoch 339 / 1000 done..\n",
            "Training Loss 0.0003911170642822981\n",
            "Epoch 340 / 1000 done..\n",
            "Training Loss 0.00039030867628753185\n",
            "Epoch 341 / 1000 done..\n",
            "Training Loss 0.0003896188864018768\n",
            "Epoch 342 / 1000 done..\n",
            "Training Loss 0.000389067514333874\n",
            "Epoch 343 / 1000 done..\n",
            "Training Loss 0.00038854338345117867\n",
            "Epoch 344 / 1000 done..\n",
            "Training Loss 0.0003881272568833083\n",
            "Epoch 345 / 1000 done..\n",
            "Training Loss 0.0003876209375448525\n",
            "Epoch 346 / 1000 done..\n",
            "Training Loss 0.0003871784429065883\n",
            "Epoch 347 / 1000 done..\n",
            "Training Loss 0.00038665192550979555\n",
            "Epoch 348 / 1000 done..\n",
            "Training Loss 0.00038617418613284826\n",
            "Epoch 349 / 1000 done..\n",
            "Training Loss 0.00038567057345062494\n",
            "Epoch 350 / 1000 done..\n",
            "Training Loss 0.0003851836663670838\n",
            "Epoch 351 / 1000 done..\n",
            "Training Loss 0.0003847090410999954\n",
            "Epoch 352 / 1000 done..\n",
            "Training Loss 0.00038421794306486845\n",
            "Epoch 353 / 1000 done..\n",
            "Training Loss 0.00038376072188839316\n",
            "Epoch 354 / 1000 done..\n",
            "Training Loss 0.0003832682268694043\n",
            "Epoch 355 / 1000 done..\n",
            "Training Loss 0.0003828269545920193\n",
            "Epoch 356 / 1000 done..\n",
            "Training Loss 0.00038233521627262235\n",
            "Epoch 357 / 1000 done..\n",
            "Training Loss 0.00038191498606465757\n",
            "Epoch 358 / 1000 done..\n",
            "Training Loss 0.00038142831181176007\n",
            "Epoch 359 / 1000 done..\n",
            "Training Loss 0.00038104684790596366\n",
            "Epoch 360 / 1000 done..\n",
            "Training Loss 0.0003805893356911838\n",
            "Epoch 361 / 1000 done..\n",
            "Training Loss 0.00038027678965590894\n",
            "Epoch 362 / 1000 done..\n",
            "Training Loss 0.00037986255483701825\n",
            "Epoch 363 / 1000 done..\n",
            "Training Loss 0.00037959188921377063\n",
            "Epoch 364 / 1000 done..\n",
            "Training Loss 0.0003791557683143765\n",
            "Epoch 365 / 1000 done..\n",
            "Training Loss 0.000378680182620883\n",
            "Epoch 366 / 1000 done..\n",
            "Training Loss 0.0003780332044698298\n",
            "Epoch 367 / 1000 done..\n",
            "Training Loss 0.00037737219827249646\n",
            "Epoch 368 / 1000 done..\n",
            "Training Loss 0.0003767507732845843\n",
            "Epoch 369 / 1000 done..\n",
            "Training Loss 0.0003761333064176142\n",
            "Epoch 370 / 1000 done..\n",
            "Training Loss 0.00037560571217909455\n",
            "Epoch 371 / 1000 done..\n",
            "Training Loss 0.0003750713658519089\n",
            "Epoch 372 / 1000 done..\n",
            "Training Loss 0.0003746075090020895\n",
            "Epoch 373 / 1000 done..\n",
            "Training Loss 0.0003741410910151899\n",
            "Epoch 374 / 1000 done..\n",
            "Training Loss 0.00037370636709965765\n",
            "Epoch 375 / 1000 done..\n",
            "Training Loss 0.0003732871264219284\n",
            "Epoch 376 / 1000 done..\n",
            "Training Loss 0.00037286168662831187\n",
            "Epoch 377 / 1000 done..\n",
            "Training Loss 0.00037248298758640885\n",
            "Epoch 378 / 1000 done..\n",
            "Training Loss 0.0003720591776072979\n",
            "Epoch 379 / 1000 done..\n",
            "Training Loss 0.00037172160227783024\n",
            "Epoch 380 / 1000 done..\n",
            "Training Loss 0.00037129633710719645\n",
            "Epoch 381 / 1000 done..\n",
            "Training Loss 0.00037097762105986476\n",
            "Epoch 382 / 1000 done..\n",
            "Training Loss 0.0003705171402543783\n",
            "Epoch 383 / 1000 done..\n",
            "Training Loss 0.00037014781264588237\n",
            "Epoch 384 / 1000 done..\n",
            "Training Loss 0.00036964204628020525\n",
            "Epoch 385 / 1000 done..\n",
            "Training Loss 0.0003692093014251441\n",
            "Epoch 386 / 1000 done..\n",
            "Training Loss 0.0003687302814796567\n",
            "Epoch 387 / 1000 done..\n",
            "Training Loss 0.0003683045506477356\n",
            "Epoch 388 / 1000 done..\n",
            "Training Loss 0.00036792701575905085\n",
            "Epoch 389 / 1000 done..\n",
            "Training Loss 0.00036747311241924763\n",
            "Epoch 390 / 1000 done..\n",
            "Training Loss 0.000367023196304217\n",
            "Epoch 391 / 1000 done..\n",
            "Training Loss 0.0003664265386760235\n",
            "Epoch 392 / 1000 done..\n",
            "Training Loss 0.00036594836274161935\n",
            "Epoch 393 / 1000 done..\n",
            "Training Loss 0.0003654045285657048\n",
            "Epoch 394 / 1000 done..\n",
            "Training Loss 0.0003649884311016649\n",
            "Epoch 395 / 1000 done..\n",
            "Training Loss 0.00036454235669225454\n",
            "Epoch 396 / 1000 done..\n",
            "Training Loss 0.00036415859358385205\n",
            "Epoch 397 / 1000 done..\n",
            "Training Loss 0.0003637880727183074\n",
            "Epoch 398 / 1000 done..\n",
            "Training Loss 0.00036340695805847645\n",
            "Epoch 399 / 1000 done..\n",
            "Training Loss 0.00036308186827227473\n",
            "Epoch 400 / 1000 done..\n",
            "Training Loss 0.00036267691757529974\n",
            "Epoch 401 / 1000 done..\n",
            "Training Loss 0.00036236364394426346\n",
            "Epoch 402 / 1000 done..\n",
            "Training Loss 0.0003619146882556379\n",
            "Epoch 403 / 1000 done..\n",
            "Training Loss 0.00036157655995339155\n",
            "Epoch 404 / 1000 done..\n",
            "Training Loss 0.00036109943175688386\n",
            "Epoch 405 / 1000 done..\n",
            "Training Loss 0.00036071392241865396\n",
            "Epoch 406 / 1000 done..\n",
            "Training Loss 0.00036025483859702945\n",
            "Epoch 407 / 1000 done..\n",
            "Training Loss 0.00035982023109681904\n",
            "Epoch 408 / 1000 done..\n",
            "Training Loss 0.00035942456452175975\n",
            "Epoch 409 / 1000 done..\n",
            "Training Loss 0.0003589589905459434\n",
            "Epoch 410 / 1000 done..\n",
            "Training Loss 0.00035864481469616294\n",
            "Epoch 411 / 1000 done..\n",
            "Training Loss 0.0003581554628908634\n",
            "Epoch 412 / 1000 done..\n",
            "Training Loss 0.0003579419862944633\n",
            "Epoch 413 / 1000 done..\n",
            "Training Loss 0.0003574503352865577\n",
            "Epoch 414 / 1000 done..\n",
            "Training Loss 0.00035741308238357306\n",
            "Epoch 415 / 1000 done..\n",
            "Training Loss 0.0003570677654352039\n",
            "Epoch 416 / 1000 done..\n",
            "Training Loss 0.000357407727278769\n",
            "Epoch 417 / 1000 done..\n",
            "Training Loss 0.00035683170426636934\n",
            "Epoch 418 / 1000 done..\n",
            "Training Loss 0.00035657003172673285\n",
            "Epoch 419 / 1000 done..\n",
            "Training Loss 0.0003554103896021843\n",
            "Epoch 420 / 1000 done..\n",
            "Training Loss 0.0003551822737790644\n",
            "Epoch 421 / 1000 done..\n",
            "Training Loss 0.0003541836922522634\n",
            "Epoch 422 / 1000 done..\n",
            "Training Loss 0.00035422382643446326\n",
            "Epoch 423 / 1000 done..\n",
            "Training Loss 0.00035333685809746385\n",
            "Epoch 424 / 1000 done..\n",
            "Training Loss 0.00035353476414456964\n",
            "Epoch 425 / 1000 done..\n",
            "Training Loss 0.0003526713699102402\n",
            "Epoch 426 / 1000 done..\n",
            "Training Loss 0.0003528982342686504\n",
            "Epoch 427 / 1000 done..\n",
            "Training Loss 0.0003520528844092041\n",
            "Epoch 428 / 1000 done..\n",
            "Training Loss 0.0003521722974255681\n",
            "Epoch 429 / 1000 done..\n",
            "Training Loss 0.00035138960811309516\n",
            "Epoch 430 / 1000 done..\n",
            "Training Loss 0.000351358437910676\n",
            "Epoch 431 / 1000 done..\n",
            "Training Loss 0.00035071722231805325\n",
            "Epoch 432 / 1000 done..\n",
            "Training Loss 0.0003505718195810914\n",
            "Epoch 433 / 1000 done..\n",
            "Training Loss 0.0003501008031889796\n",
            "Epoch 434 / 1000 done..\n",
            "Training Loss 0.0003498744044918567\n",
            "Epoch 435 / 1000 done..\n",
            "Training Loss 0.0003495722485240549\n",
            "Epoch 436 / 1000 done..\n",
            "Training Loss 0.0003492796968203038\n",
            "Epoch 437 / 1000 done..\n",
            "Training Loss 0.0003491631359793246\n",
            "Epoch 438 / 1000 done..\n",
            "Training Loss 0.00034881033934652805\n",
            "Epoch 439 / 1000 done..\n",
            "Training Loss 0.00034889637026935816\n",
            "Epoch 440 / 1000 done..\n",
            "Training Loss 0.0003484339395072311\n",
            "Epoch 441 / 1000 done..\n",
            "Training Loss 0.00034843329922296107\n",
            "Epoch 442 / 1000 done..\n",
            "Training Loss 0.00034762424184009433\n",
            "Epoch 443 / 1000 done..\n",
            "Training Loss 0.0003473993274383247\n",
            "Epoch 444 / 1000 done..\n",
            "Training Loss 0.00034656893694773316\n",
            "Epoch 445 / 1000 done..\n",
            "Training Loss 0.00034638855140656233\n",
            "Epoch 446 / 1000 done..\n",
            "Training Loss 0.0003457177081145346\n",
            "Epoch 447 / 1000 done..\n",
            "Training Loss 0.00034559768391773105\n",
            "Epoch 448 / 1000 done..\n",
            "Training Loss 0.00034506566589698195\n",
            "Epoch 449 / 1000 done..\n",
            "Training Loss 0.00034494756255298853\n",
            "Epoch 450 / 1000 done..\n",
            "Training Loss 0.0003445560287218541\n",
            "Epoch 451 / 1000 done..\n",
            "Training Loss 0.00034438312286511064\n",
            "Epoch 452 / 1000 done..\n",
            "Training Loss 0.0003441378357820213\n",
            "Epoch 453 / 1000 done..\n",
            "Training Loss 0.00034389080246910453\n",
            "Epoch 454 / 1000 done..\n",
            "Training Loss 0.00034368032356724143\n",
            "Epoch 455 / 1000 done..\n",
            "Training Loss 0.00034338602563366294\n",
            "Epoch 456 / 1000 done..\n",
            "Training Loss 0.00034301247796975076\n",
            "Epoch 457 / 1000 done..\n",
            "Training Loss 0.00034280665568076074\n",
            "Epoch 458 / 1000 done..\n",
            "Training Loss 0.0003422512672841549\n",
            "Epoch 459 / 1000 done..\n",
            "Training Loss 0.0003422203881200403\n",
            "Epoch 460 / 1000 done..\n",
            "Training Loss 0.0003415931132622063\n",
            "Epoch 461 / 1000 done..\n",
            "Training Loss 0.0003416802210267633\n",
            "Epoch 462 / 1000 done..\n",
            "Training Loss 0.0003411881043575704\n",
            "Epoch 463 / 1000 done..\n",
            "Training Loss 0.00034143595257773995\n",
            "Epoch 464 / 1000 done..\n",
            "Training Loss 0.00034133391454815865\n",
            "Epoch 465 / 1000 done..\n",
            "Training Loss 0.0003413578087929636\n",
            "Epoch 466 / 1000 done..\n",
            "Training Loss 0.0003405049501452595\n",
            "Epoch 467 / 1000 done..\n",
            "Training Loss 0.00034019461600109935\n",
            "Epoch 468 / 1000 done..\n",
            "Training Loss 0.0003392600337974727\n",
            "Epoch 469 / 1000 done..\n",
            "Training Loss 0.00033938902197405696\n",
            "Epoch 470 / 1000 done..\n",
            "Training Loss 0.0003385067393537611\n",
            "Epoch 471 / 1000 done..\n",
            "Training Loss 0.0003389237099327147\n",
            "Epoch 472 / 1000 done..\n",
            "Training Loss 0.00033800239907577634\n",
            "Epoch 473 / 1000 done..\n",
            "Training Loss 0.00033846369478851557\n",
            "Epoch 474 / 1000 done..\n",
            "Training Loss 0.00033751962473616004\n",
            "Epoch 475 / 1000 done..\n",
            "Training Loss 0.0003379054833203554\n",
            "Epoch 476 / 1000 done..\n",
            "Training Loss 0.0003370320191606879\n",
            "Epoch 477 / 1000 done..\n",
            "Training Loss 0.00033734217868186533\n",
            "Epoch 478 / 1000 done..\n",
            "Training Loss 0.00033657136373221874\n",
            "Epoch 479 / 1000 done..\n",
            "Training Loss 0.0003368325997143984\n",
            "Epoch 480 / 1000 done..\n",
            "Training Loss 0.000336138007696718\n",
            "Epoch 481 / 1000 done..\n",
            "Training Loss 0.00033637561136856675\n",
            "Epoch 482 / 1000 done..\n",
            "Training Loss 0.00033573078690096736\n",
            "Epoch 483 / 1000 done..\n",
            "Training Loss 0.0003359558177180588\n",
            "Epoch 484 / 1000 done..\n",
            "Training Loss 0.00033534696558490396\n",
            "Epoch 485 / 1000 done..\n",
            "Training Loss 0.0003355722001288086\n",
            "Epoch 486 / 1000 done..\n",
            "Training Loss 0.00033499259734526277\n",
            "Epoch 487 / 1000 done..\n",
            "Training Loss 0.0003351752820890397\n",
            "Epoch 488 / 1000 done..\n",
            "Training Loss 0.0003344835131429136\n",
            "Epoch 489 / 1000 done..\n",
            "Training Loss 0.0003345636068843305\n",
            "Epoch 490 / 1000 done..\n",
            "Training Loss 0.00033370329765602946\n",
            "Epoch 491 / 1000 done..\n",
            "Training Loss 0.00033382856054231524\n",
            "Epoch 492 / 1000 done..\n",
            "Training Loss 0.0003329257306177169\n",
            "Epoch 493 / 1000 done..\n",
            "Training Loss 0.0003332043415866792\n",
            "Epoch 494 / 1000 done..\n",
            "Training Loss 0.000332308845827356\n",
            "Epoch 495 / 1000 done..\n",
            "Training Loss 0.00033272523432970047\n",
            "Epoch 496 / 1000 done..\n",
            "Training Loss 0.0003318143426440656\n",
            "Epoch 497 / 1000 done..\n",
            "Training Loss 0.0003323286946397275\n",
            "Epoch 498 / 1000 done..\n",
            "Training Loss 0.00033138220896944404\n",
            "Epoch 499 / 1000 done..\n",
            "Training Loss 0.0003319734241813421\n",
            "Epoch 500 / 1000 done..\n",
            "Training Loss 0.00033097874256782234\n",
            "Epoch 501 / 1000 done..\n",
            "Training Loss 0.00033163471380248666\n",
            "Epoch 502 / 1000 done..\n",
            "Training Loss 0.0003305892751086503\n",
            "Epoch 503 / 1000 done..\n",
            "Training Loss 0.0003313024644739926\n",
            "Epoch 504 / 1000 done..\n",
            "Training Loss 0.000330205075442791\n",
            "Epoch 505 / 1000 done..\n",
            "Training Loss 0.0003309455933049321\n",
            "Epoch 506 / 1000 done..\n",
            "Training Loss 0.00032977439695969224\n",
            "Epoch 507 / 1000 done..\n",
            "Training Loss 0.00033048581099137664\n",
            "Epoch 508 / 1000 done..\n",
            "Training Loss 0.00032922346144914627\n",
            "Epoch 509 / 1000 done..\n",
            "Training Loss 0.00032986304722726345\n",
            "Epoch 510 / 1000 done..\n",
            "Training Loss 0.0003285434504505247\n",
            "Epoch 511 / 1000 done..\n",
            "Training Loss 0.0003291448228992522\n",
            "Epoch 512 / 1000 done..\n",
            "Training Loss 0.0003278659423813224\n",
            "Epoch 513 / 1000 done..\n",
            "Training Loss 0.00032845366513356566\n",
            "Epoch 514 / 1000 done..\n",
            "Training Loss 0.0003272665198892355\n",
            "Epoch 515 / 1000 done..\n",
            "Training Loss 0.00032782176276668906\n",
            "Epoch 516 / 1000 done..\n",
            "Training Loss 0.000326743524055928\n",
            "Epoch 517 / 1000 done..\n",
            "Training Loss 0.0003272395406384021\n",
            "Epoch 518 / 1000 done..\n",
            "Training Loss 0.000326276000123471\n",
            "Epoch 519 / 1000 done..\n",
            "Training Loss 0.0003266974235884845\n",
            "Epoch 520 / 1000 done..\n",
            "Training Loss 0.0003258525102864951\n",
            "Epoch 521 / 1000 done..\n",
            "Training Loss 0.0003261961683165282\n",
            "Epoch 522 / 1000 done..\n",
            "Training Loss 0.00032544549321755767\n",
            "Epoch 523 / 1000 done..\n",
            "Training Loss 0.00032571604242548347\n",
            "Epoch 524 / 1000 done..\n",
            "Training Loss 0.0003250176378060132\n",
            "Epoch 525 / 1000 done..\n",
            "Training Loss 0.00032523012487217784\n",
            "Epoch 526 / 1000 done..\n",
            "Training Loss 0.0003245298867113888\n",
            "Epoch 527 / 1000 done..\n",
            "Training Loss 0.0003247370186727494\n",
            "Epoch 528 / 1000 done..\n",
            "Training Loss 0.00032395421294495463\n",
            "Epoch 529 / 1000 done..\n",
            "Training Loss 0.00032424909295514226\n",
            "Epoch 530 / 1000 done..\n",
            "Training Loss 0.0003233320312574506\n",
            "Epoch 531 / 1000 done..\n",
            "Training Loss 0.0003237599739804864\n",
            "Epoch 532 / 1000 done..\n",
            "Training Loss 0.00032264902256429195\n",
            "Epoch 533 / 1000 done..\n",
            "Training Loss 0.0003230809816159308\n",
            "Epoch 534 / 1000 done..\n",
            "Training Loss 0.0003218379570171237\n",
            "Epoch 535 / 1000 done..\n",
            "Training Loss 0.000322268926538527\n",
            "Epoch 536 / 1000 done..\n",
            "Training Loss 0.0003210841096006334\n",
            "Epoch 537 / 1000 done..\n",
            "Training Loss 0.00032150745391845703\n",
            "Epoch 538 / 1000 done..\n",
            "Training Loss 0.0003204651875421405\n",
            "Epoch 539 / 1000 done..\n",
            "Training Loss 0.00032083509722724557\n",
            "Epoch 540 / 1000 done..\n",
            "Training Loss 0.00031996023608371615\n",
            "Epoch 541 / 1000 done..\n",
            "Training Loss 0.00032022898085415363\n",
            "Epoch 542 / 1000 done..\n",
            "Training Loss 0.0003195077588316053\n",
            "Epoch 543 / 1000 done..\n",
            "Training Loss 0.00031963514629751444\n",
            "Epoch 544 / 1000 done..\n",
            "Training Loss 0.00031899817986413836\n",
            "Epoch 545 / 1000 done..\n",
            "Training Loss 0.00031895103165879846\n",
            "Epoch 546 / 1000 done..\n",
            "Training Loss 0.0003183581866323948\n",
            "Epoch 547 / 1000 done..\n",
            "Training Loss 0.0003181541687808931\n",
            "Epoch 548 / 1000 done..\n",
            "Training Loss 0.0003176021564286202\n",
            "Epoch 549 / 1000 done..\n",
            "Training Loss 0.0003172704018652439\n",
            "Epoch 550 / 1000 done..\n",
            "Training Loss 0.0003167616087011993\n",
            "Epoch 551 / 1000 done..\n",
            "Training Loss 0.00031637269421480596\n",
            "Epoch 552 / 1000 done..\n",
            "Training Loss 0.0003159213229082525\n",
            "Epoch 553 / 1000 done..\n",
            "Training Loss 0.0003155697195325047\n",
            "Epoch 554 / 1000 done..\n",
            "Training Loss 0.00031515874434262514\n",
            "Epoch 555 / 1000 done..\n",
            "Training Loss 0.00031493051210418344\n",
            "Epoch 556 / 1000 done..\n",
            "Training Loss 0.00031452334951609373\n",
            "Epoch 557 / 1000 done..\n",
            "Training Loss 0.0003144710208289325\n",
            "Epoch 558 / 1000 done..\n",
            "Training Loss 0.00031397101702168584\n",
            "Epoch 559 / 1000 done..\n",
            "Training Loss 0.0003138926695100963\n",
            "Epoch 560 / 1000 done..\n",
            "Training Loss 0.0003131969424430281\n",
            "Epoch 561 / 1000 done..\n",
            "Training Loss 0.0003130104742012918\n",
            "Epoch 562 / 1000 done..\n",
            "Training Loss 0.00031227318686433136\n",
            "Epoch 563 / 1000 done..\n",
            "Training Loss 0.0003120483597740531\n",
            "Epoch 564 / 1000 done..\n",
            "Training Loss 0.00031133851734921336\n",
            "Epoch 565 / 1000 done..\n",
            "Training Loss 0.0003110399120487273\n",
            "Epoch 566 / 1000 done..\n",
            "Training Loss 0.0003103782655671239\n",
            "Epoch 567 / 1000 done..\n",
            "Training Loss 0.0003099677269347012\n",
            "Epoch 568 / 1000 done..\n",
            "Training Loss 0.00030941751901991665\n",
            "Epoch 569 / 1000 done..\n",
            "Training Loss 0.00030891012283973396\n",
            "Epoch 570 / 1000 done..\n",
            "Training Loss 0.0003084843046963215\n",
            "Epoch 571 / 1000 done..\n",
            "Training Loss 0.0003079119196627289\n",
            "Epoch 572 / 1000 done..\n",
            "Training Loss 0.0003075486747547984\n",
            "Epoch 573 / 1000 done..\n",
            "Training Loss 0.0003069469821639359\n",
            "Epoch 574 / 1000 done..\n",
            "Training Loss 0.0003065986675210297\n",
            "Epoch 575 / 1000 done..\n",
            "Training Loss 0.0003059835289604962\n",
            "Epoch 576 / 1000 done..\n",
            "Training Loss 0.00030565145425498486\n",
            "Epoch 577 / 1000 done..\n",
            "Training Loss 0.0003049850929528475\n",
            "Epoch 578 / 1000 done..\n",
            "Training Loss 0.0003047156205866486\n",
            "Epoch 579 / 1000 done..\n",
            "Training Loss 0.0003039569710381329\n",
            "Epoch 580 / 1000 done..\n",
            "Training Loss 0.00030379652162082493\n",
            "Epoch 581 / 1000 done..\n",
            "Training Loss 0.0003029277140740305\n",
            "Epoch 582 / 1000 done..\n",
            "Training Loss 0.00030287905246950686\n",
            "Epoch 583 / 1000 done..\n",
            "Training Loss 0.0003018923453055322\n",
            "Epoch 584 / 1000 done..\n",
            "Training Loss 0.00030190637335181236\n",
            "Epoch 585 / 1000 done..\n",
            "Training Loss 0.00030081559089012444\n",
            "Epoch 586 / 1000 done..\n",
            "Training Loss 0.00030096681439317763\n",
            "Epoch 587 / 1000 done..\n",
            "Training Loss 0.00029984800494275987\n",
            "Epoch 588 / 1000 done..\n",
            "Training Loss 0.0003005671896971762\n",
            "Epoch 589 / 1000 done..\n",
            "Training Loss 0.00029894890030846\n",
            "Epoch 590 / 1000 done..\n",
            "Training Loss 0.00029923528200015426\n",
            "Epoch 591 / 1000 done..\n",
            "Training Loss 0.0002966911706607789\n",
            "Epoch 592 / 1000 done..\n",
            "Training Loss 0.00029705686029046774\n",
            "Epoch 593 / 1000 done..\n",
            "Training Loss 0.0002947383909486234\n",
            "Epoch 594 / 1000 done..\n",
            "Training Loss 0.00029531202744692564\n",
            "Epoch 595 / 1000 done..\n",
            "Training Loss 0.00029337313026189804\n",
            "Epoch 596 / 1000 done..\n",
            "Training Loss 0.00029390145209617913\n",
            "Epoch 597 / 1000 done..\n",
            "Training Loss 0.0002922816202044487\n",
            "Epoch 598 / 1000 done..\n",
            "Training Loss 0.00029262781026773155\n",
            "Epoch 599 / 1000 done..\n",
            "Training Loss 0.0002912448544520885\n",
            "Epoch 600 / 1000 done..\n",
            "Training Loss 0.00029138807440176606\n",
            "Epoch 601 / 1000 done..\n",
            "Training Loss 0.00029017170891165733\n",
            "Epoch 602 / 1000 done..\n",
            "Training Loss 0.0002901454281527549\n",
            "Epoch 603 / 1000 done..\n",
            "Training Loss 0.00028903959901072085\n",
            "Epoch 604 / 1000 done..\n",
            "Training Loss 0.00028886698419228196\n",
            "Epoch 605 / 1000 done..\n",
            "Training Loss 0.0002878168597817421\n",
            "Epoch 606 / 1000 done..\n",
            "Training Loss 0.00028745009331032634\n",
            "Epoch 607 / 1000 done..\n",
            "Training Loss 0.0002864280832000077\n",
            "Epoch 608 / 1000 done..\n",
            "Training Loss 0.00028582493541762233\n",
            "Epoch 609 / 1000 done..\n",
            "Training Loss 0.0002848755684681237\n",
            "Epoch 610 / 1000 done..\n",
            "Training Loss 0.00028409657534211874\n",
            "Epoch 611 / 1000 done..\n",
            "Training Loss 0.00028330384520813823\n",
            "Epoch 612 / 1000 done..\n",
            "Training Loss 0.000282433582469821\n",
            "Epoch 613 / 1000 done..\n",
            "Training Loss 0.0002818050270434469\n",
            "Epoch 614 / 1000 done..\n",
            "Training Loss 0.0002808990248013288\n",
            "Epoch 615 / 1000 done..\n",
            "Training Loss 0.00028034087154082954\n",
            "Epoch 616 / 1000 done..\n",
            "Training Loss 0.00027940815198235214\n",
            "Epoch 617 / 1000 done..\n",
            "Training Loss 0.00027874193619936705\n",
            "Epoch 618 / 1000 done..\n",
            "Training Loss 0.0002778023190330714\n",
            "Epoch 619 / 1000 done..\n",
            "Training Loss 0.000276945938821882\n",
            "Epoch 620 / 1000 done..\n",
            "Training Loss 0.0002760516945272684\n",
            "Epoch 621 / 1000 done..\n",
            "Training Loss 0.00027506775222718716\n",
            "Epoch 622 / 1000 done..\n",
            "Training Loss 0.0002741855278145522\n",
            "Epoch 623 / 1000 done..\n",
            "Training Loss 0.00027320923982188106\n",
            "Epoch 624 / 1000 done..\n",
            "Training Loss 0.0002723005018197\n",
            "Epoch 625 / 1000 done..\n",
            "Training Loss 0.0002714282600209117\n",
            "Epoch 626 / 1000 done..\n",
            "Training Loss 0.0002704799990169704\n",
            "Epoch 627 / 1000 done..\n",
            "Training Loss 0.00026965801953338087\n",
            "Epoch 628 / 1000 done..\n",
            "Training Loss 0.00026860489742830396\n",
            "Epoch 629 / 1000 done..\n",
            "Training Loss 0.0002677410957403481\n",
            "Epoch 630 / 1000 done..\n",
            "Training Loss 0.00026657688431441784\n",
            "Epoch 631 / 1000 done..\n",
            "Training Loss 0.00026569448527880013\n",
            "Epoch 632 / 1000 done..\n",
            "Training Loss 0.0002645213098730892\n",
            "Epoch 633 / 1000 done..\n",
            "Training Loss 0.00026367948157712817\n",
            "Epoch 634 / 1000 done..\n",
            "Training Loss 0.00026255432749167085\n",
            "Epoch 635 / 1000 done..\n",
            "Training Loss 0.00026173453079536557\n",
            "Epoch 636 / 1000 done..\n",
            "Training Loss 0.0002606580383144319\n",
            "Epoch 637 / 1000 done..\n",
            "Training Loss 0.0002598236605990678\n",
            "Epoch 638 / 1000 done..\n",
            "Training Loss 0.0002587826456874609\n",
            "Epoch 639 / 1000 done..\n",
            "Training Loss 0.0002579018473625183\n",
            "Epoch 640 / 1000 done..\n",
            "Training Loss 0.00025689127505756915\n",
            "Epoch 641 / 1000 done..\n",
            "Training Loss 0.0002559603308327496\n",
            "Epoch 642 / 1000 done..\n",
            "Training Loss 0.0002549855562392622\n",
            "Epoch 643 / 1000 done..\n",
            "Training Loss 0.00025401555467396975\n",
            "Epoch 644 / 1000 done..\n",
            "Training Loss 0.00025306345196440816\n",
            "Epoch 645 / 1000 done..\n",
            "Training Loss 0.0002520785783417523\n",
            "Epoch 646 / 1000 done..\n",
            "Training Loss 0.00025105237727984786\n",
            "Epoch 647 / 1000 done..\n",
            "Training Loss 0.00025005993666127324\n",
            "Epoch 648 / 1000 done..\n",
            "Training Loss 0.00024877028772607446\n",
            "Epoch 649 / 1000 done..\n",
            "Training Loss 0.00024777770158834755\n",
            "Epoch 650 / 1000 done..\n",
            "Training Loss 0.0002461750991642475\n",
            "Epoch 651 / 1000 done..\n",
            "Training Loss 0.00024527980713173747\n",
            "Epoch 652 / 1000 done..\n",
            "Training Loss 0.00024353220942430198\n",
            "Epoch 653 / 1000 done..\n",
            "Training Loss 0.00024274690076708794\n",
            "Epoch 654 / 1000 done..\n",
            "Training Loss 0.0002410343149676919\n",
            "Epoch 655 / 1000 done..\n",
            "Training Loss 0.00024033164663705975\n",
            "Epoch 656 / 1000 done..\n",
            "Training Loss 0.0002387008280493319\n",
            "Epoch 657 / 1000 done..\n",
            "Training Loss 0.00023805396631360054\n",
            "Epoch 658 / 1000 done..\n",
            "Training Loss 0.00023647097987122834\n",
            "Epoch 659 / 1000 done..\n",
            "Training Loss 0.0002358490601181984\n",
            "Epoch 660 / 1000 done..\n",
            "Training Loss 0.00023425676045008004\n",
            "Epoch 661 / 1000 done..\n",
            "Training Loss 0.0002336434554308653\n",
            "Epoch 662 / 1000 done..\n",
            "Training Loss 0.00023202438023872674\n",
            "Epoch 663 / 1000 done..\n",
            "Training Loss 0.00023140053963288665\n",
            "Epoch 664 / 1000 done..\n",
            "Training Loss 0.00022976615582592785\n",
            "Epoch 665 / 1000 done..\n",
            "Training Loss 0.0002291198179591447\n",
            "Epoch 666 / 1000 done..\n",
            "Training Loss 0.00022750165953766555\n",
            "Epoch 667 / 1000 done..\n",
            "Training Loss 0.00022681083646602929\n",
            "Epoch 668 / 1000 done..\n",
            "Training Loss 0.0002251945115858689\n",
            "Epoch 669 / 1000 done..\n",
            "Training Loss 0.0002243713242933154\n",
            "Epoch 670 / 1000 done..\n",
            "Training Loss 0.00022268816246651113\n",
            "Epoch 671 / 1000 done..\n",
            "Training Loss 0.00022170277952682227\n",
            "Epoch 672 / 1000 done..\n",
            "Training Loss 0.0002200456365244463\n",
            "Epoch 673 / 1000 done..\n",
            "Training Loss 0.00021901802392676473\n",
            "Epoch 674 / 1000 done..\n",
            "Training Loss 0.00021749369625467807\n",
            "Epoch 675 / 1000 done..\n",
            "Training Loss 0.0002164696343243122\n",
            "Epoch 676 / 1000 done..\n",
            "Training Loss 0.0002150704967789352\n",
            "Epoch 677 / 1000 done..\n",
            "Training Loss 0.00021403163555078208\n",
            "Epoch 678 / 1000 done..\n",
            "Training Loss 0.00021269623539410532\n",
            "Epoch 679 / 1000 done..\n",
            "Training Loss 0.00021163719065953046\n",
            "Epoch 680 / 1000 done..\n",
            "Training Loss 0.00021030547213740647\n",
            "Epoch 681 / 1000 done..\n",
            "Training Loss 0.00020925112767145038\n",
            "Epoch 682 / 1000 done..\n",
            "Training Loss 0.00020788225810974836\n",
            "Epoch 683 / 1000 done..\n",
            "Training Loss 0.00020687768119387329\n",
            "Epoch 684 / 1000 done..\n",
            "Training Loss 0.00020544820290524513\n",
            "Epoch 685 / 1000 done..\n",
            "Training Loss 0.00020452775061130524\n",
            "Epoch 686 / 1000 done..\n",
            "Training Loss 0.00020304942154325545\n",
            "Epoch 687 / 1000 done..\n",
            "Training Loss 0.00020223576575517654\n",
            "Epoch 688 / 1000 done..\n",
            "Training Loss 0.0002007288276217878\n",
            "Epoch 689 / 1000 done..\n",
            "Training Loss 0.0001999400556087494\n",
            "Epoch 690 / 1000 done..\n",
            "Training Loss 0.00019827447249554098\n",
            "Epoch 691 / 1000 done..\n",
            "Training Loss 0.00019730276835616678\n",
            "Epoch 692 / 1000 done..\n",
            "Training Loss 0.00019553839229047298\n",
            "Epoch 693 / 1000 done..\n",
            "Training Loss 0.0001945599215105176\n",
            "Epoch 694 / 1000 done..\n",
            "Training Loss 0.00019293357036076486\n",
            "Epoch 695 / 1000 done..\n",
            "Training Loss 0.00019206729484722018\n",
            "Epoch 696 / 1000 done..\n",
            "Training Loss 0.00019057883764617145\n",
            "Epoch 697 / 1000 done..\n",
            "Training Loss 0.00018979146261699498\n",
            "Epoch 698 / 1000 done..\n",
            "Training Loss 0.00018836537492461503\n",
            "Epoch 699 / 1000 done..\n",
            "Training Loss 0.00018761891988106072\n",
            "Epoch 700 / 1000 done..\n",
            "Training Loss 0.00018619716865941882\n",
            "Epoch 701 / 1000 done..\n",
            "Training Loss 0.0001854808651842177\n",
            "Epoch 702 / 1000 done..\n",
            "Training Loss 0.00018403149442747235\n",
            "Epoch 703 / 1000 done..\n",
            "Training Loss 0.00018334953347221017\n",
            "Epoch 704 / 1000 done..\n",
            "Training Loss 0.00018186891975346953\n",
            "Epoch 705 / 1000 done..\n",
            "Training Loss 0.00018121808534488082\n",
            "Epoch 706 / 1000 done..\n",
            "Training Loss 0.00017972172645386308\n",
            "Epoch 707 / 1000 done..\n",
            "Training Loss 0.00017910354654304683\n",
            "Epoch 708 / 1000 done..\n",
            "Training Loss 0.00017763680079951882\n",
            "Epoch 709 / 1000 done..\n",
            "Training Loss 0.00017709638632368296\n",
            "Epoch 710 / 1000 done..\n",
            "Training Loss 0.00017580886196810752\n",
            "Epoch 711 / 1000 done..\n",
            "Training Loss 0.000175490538822487\n",
            "Epoch 712 / 1000 done..\n",
            "Training Loss 0.00017422136443201452\n",
            "Epoch 713 / 1000 done..\n",
            "Training Loss 0.0001732855598675087\n",
            "Epoch 714 / 1000 done..\n",
            "Training Loss 0.00017164672317449003\n",
            "Epoch 715 / 1000 done..\n",
            "Training Loss 0.00017055770149454474\n",
            "Epoch 716 / 1000 done..\n",
            "Training Loss 0.0001693509257165715\n",
            "Epoch 717 / 1000 done..\n",
            "Training Loss 0.0001684402086539194\n",
            "Epoch 718 / 1000 done..\n",
            "Training Loss 0.00016756114200688899\n",
            "Epoch 719 / 1000 done..\n",
            "Training Loss 0.000166737285326235\n",
            "Epoch 720 / 1000 done..\n",
            "Training Loss 0.00016600207891315222\n",
            "Epoch 721 / 1000 done..\n",
            "Training Loss 0.00016519935161340982\n",
            "Epoch 722 / 1000 done..\n",
            "Training Loss 0.00016449784743599594\n",
            "Epoch 723 / 1000 done..\n",
            "Training Loss 0.00016369755030609667\n",
            "Epoch 724 / 1000 done..\n",
            "Training Loss 0.0001629510661587119\n",
            "Epoch 725 / 1000 done..\n",
            "Training Loss 0.0001621381816221401\n",
            "Epoch 726 / 1000 done..\n",
            "Training Loss 0.0001613151398487389\n",
            "Epoch 727 / 1000 done..\n",
            "Training Loss 0.00016050308477133512\n",
            "Epoch 728 / 1000 done..\n",
            "Training Loss 0.00015966943465173244\n",
            "Epoch 729 / 1000 done..\n",
            "Training Loss 0.00015892017108853906\n",
            "Epoch 730 / 1000 done..\n",
            "Training Loss 0.00015822042769286782\n",
            "Epoch 731 / 1000 done..\n",
            "Training Loss 0.00015756036737002432\n",
            "Epoch 732 / 1000 done..\n",
            "Training Loss 0.00015681420336477458\n",
            "Epoch 733 / 1000 done..\n",
            "Training Loss 0.000155947869643569\n",
            "Epoch 734 / 1000 done..\n",
            "Training Loss 0.00015498374705202878\n",
            "Epoch 735 / 1000 done..\n",
            "Training Loss 0.000154203298734501\n",
            "Epoch 736 / 1000 done..\n",
            "Training Loss 0.00015334266936406493\n",
            "Epoch 737 / 1000 done..\n",
            "Training Loss 0.0001528052962385118\n",
            "Epoch 738 / 1000 done..\n",
            "Training Loss 0.00015207078831735998\n",
            "Epoch 739 / 1000 done..\n",
            "Training Loss 0.0001516906195320189\n",
            "Epoch 740 / 1000 done..\n",
            "Training Loss 0.00015097690629772842\n",
            "Epoch 741 / 1000 done..\n",
            "Training Loss 0.00015064039325807244\n",
            "Epoch 742 / 1000 done..\n",
            "Training Loss 0.00014984884182922542\n",
            "Epoch 743 / 1000 done..\n",
            "Training Loss 0.000149513129144907\n",
            "Epoch 744 / 1000 done..\n",
            "Training Loss 0.0001486513647250831\n",
            "Epoch 745 / 1000 done..\n",
            "Training Loss 0.00014836910122539848\n",
            "Epoch 746 / 1000 done..\n",
            "Training Loss 0.00014749885303899646\n",
            "Epoch 747 / 1000 done..\n",
            "Training Loss 0.0001473168085794896\n",
            "Epoch 748 / 1000 done..\n",
            "Training Loss 0.00014648379874415696\n",
            "Epoch 749 / 1000 done..\n",
            "Training Loss 0.00014639760775025934\n",
            "Epoch 750 / 1000 done..\n",
            "Training Loss 0.0001456175377825275\n",
            "Epoch 751 / 1000 done..\n",
            "Training Loss 0.00014554204244632274\n",
            "Epoch 752 / 1000 done..\n",
            "Training Loss 0.00014470376481767744\n",
            "Epoch 753 / 1000 done..\n",
            "Training Loss 0.00014451597235165536\n",
            "Epoch 754 / 1000 done..\n",
            "Training Loss 0.00014360951900016516\n",
            "Epoch 755 / 1000 done..\n",
            "Training Loss 0.00014339365588966757\n",
            "Epoch 756 / 1000 done..\n",
            "Training Loss 0.00014258731971494853\n",
            "Epoch 757 / 1000 done..\n",
            "Training Loss 0.0001424075016984716\n",
            "Epoch 758 / 1000 done..\n",
            "Training Loss 0.0001417681050952524\n",
            "Epoch 759 / 1000 done..\n",
            "Training Loss 0.00014159511192701757\n",
            "Epoch 760 / 1000 done..\n",
            "Training Loss 0.00014111734344623983\n",
            "Epoch 761 / 1000 done..\n",
            "Training Loss 0.00014090584591031075\n",
            "Epoch 762 / 1000 done..\n",
            "Training Loss 0.00014054926577955484\n",
            "Epoch 763 / 1000 done..\n",
            "Training Loss 0.00014027516590431333\n",
            "Epoch 764 / 1000 done..\n",
            "Training Loss 0.00013996206689625978\n",
            "Epoch 765 / 1000 done..\n",
            "Training Loss 0.00013964282697997987\n",
            "Epoch 766 / 1000 done..\n",
            "Training Loss 0.0001392537960782647\n",
            "Epoch 767 / 1000 done..\n",
            "Training Loss 0.00013900530757382512\n",
            "Epoch 768 / 1000 done..\n",
            "Training Loss 0.00013847570517100394\n",
            "Epoch 769 / 1000 done..\n",
            "Training Loss 0.00013847181980963796\n",
            "Epoch 770 / 1000 done..\n",
            "Training Loss 0.00013776635751128197\n",
            "Epoch 771 / 1000 done..\n",
            "Training Loss 0.00013806740753352642\n",
            "Epoch 772 / 1000 done..\n",
            "Training Loss 0.00013713250518776476\n",
            "Epoch 773 / 1000 done..\n",
            "Training Loss 0.0001375743595417589\n",
            "Epoch 774 / 1000 done..\n",
            "Training Loss 0.00013644504360854626\n",
            "Epoch 775 / 1000 done..\n",
            "Training Loss 0.00013679124822374433\n",
            "Epoch 776 / 1000 done..\n",
            "Training Loss 0.00013570072769653052\n",
            "Epoch 777 / 1000 done..\n",
            "Training Loss 0.00013596986536867917\n",
            "Epoch 778 / 1000 done..\n",
            "Training Loss 0.00013508356641978025\n",
            "Epoch 779 / 1000 done..\n",
            "Training Loss 0.00013533842866308987\n",
            "Epoch 780 / 1000 done..\n",
            "Training Loss 0.0001346351782558486\n",
            "Epoch 781 / 1000 done..\n",
            "Training Loss 0.00013489217963069677\n",
            "Epoch 782 / 1000 done..\n",
            "Training Loss 0.00013430067338049412\n",
            "Epoch 783 / 1000 done..\n",
            "Training Loss 0.00013455971202347428\n",
            "Epoch 784 / 1000 done..\n",
            "Training Loss 0.00013402351760305464\n",
            "Epoch 785 / 1000 done..\n",
            "Training Loss 0.00013428815873339772\n",
            "Epoch 786 / 1000 done..\n",
            "Training Loss 0.0001337713038083166\n",
            "Epoch 787 / 1000 done..\n",
            "Training Loss 0.00013406398647930473\n",
            "Epoch 788 / 1000 done..\n",
            "Training Loss 0.0001335285633103922\n",
            "Epoch 789 / 1000 done..\n",
            "Training Loss 0.00013387207582127303\n",
            "Epoch 790 / 1000 done..\n",
            "Training Loss 0.00013324423343874514\n",
            "Epoch 791 / 1000 done..\n",
            "Training Loss 0.00013370500528253615\n",
            "Epoch 792 / 1000 done..\n",
            "Training Loss 0.00013292298535816371\n",
            "Epoch 793 / 1000 done..\n",
            "Training Loss 0.00013368652435019612\n",
            "Epoch 794 / 1000 done..\n",
            "Training Loss 0.00013280402345117182\n",
            "Epoch 795 / 1000 done..\n",
            "Training Loss 0.0001337923458777368\n",
            "Epoch 796 / 1000 done..\n",
            "Training Loss 0.00013250534539110959\n",
            "Epoch 797 / 1000 done..\n",
            "Training Loss 0.00013306448818184435\n",
            "Epoch 798 / 1000 done..\n",
            "Training Loss 0.00013168320583645254\n",
            "Epoch 799 / 1000 done..\n",
            "Training Loss 0.0001321474846918136\n",
            "Epoch 800 / 1000 done..\n",
            "Training Loss 0.0001310851366724819\n",
            "Epoch 801 / 1000 done..\n",
            "Training Loss 0.00013156300701666623\n",
            "Epoch 802 / 1000 done..\n",
            "Training Loss 0.00013079307973384857\n",
            "Epoch 803 / 1000 done..\n",
            "Training Loss 0.00013127861893735826\n",
            "Epoch 804 / 1000 done..\n",
            "Training Loss 0.0001307029597228393\n",
            "Epoch 805 / 1000 done..\n",
            "Training Loss 0.00013119151117280126\n",
            "Epoch 806 / 1000 done..\n",
            "Training Loss 0.00013070581189822406\n",
            "Epoch 807 / 1000 done..\n",
            "Training Loss 0.0001311907690251246\n",
            "Epoch 808 / 1000 done..\n",
            "Training Loss 0.00013069658598396927\n",
            "Epoch 809 / 1000 done..\n",
            "Training Loss 0.00013118248898535967\n",
            "Epoch 810 / 1000 done..\n",
            "Training Loss 0.00013060198398306966\n",
            "Epoch 811 / 1000 done..\n",
            "Training Loss 0.00013112749729771167\n",
            "Epoch 812 / 1000 done..\n",
            "Training Loss 0.0001304494944633916\n",
            "Epoch 813 / 1000 done..\n",
            "Training Loss 0.00013104660320095718\n",
            "Epoch 814 / 1000 done..\n",
            "Training Loss 0.00013027354725636542\n",
            "Epoch 815 / 1000 done..\n",
            "Training Loss 0.00013091755681671202\n",
            "Epoch 816 / 1000 done..\n",
            "Training Loss 0.00013006269000470638\n",
            "Epoch 817 / 1000 done..\n",
            "Training Loss 0.00013070582645013928\n",
            "Epoch 818 / 1000 done..\n",
            "Training Loss 0.00012980724568478763\n",
            "Epoch 819 / 1000 done..\n",
            "Training Loss 0.00013045445666648448\n",
            "Epoch 820 / 1000 done..\n",
            "Training Loss 0.00012957293074578047\n",
            "Epoch 821 / 1000 done..\n",
            "Training Loss 0.0001302727614529431\n",
            "Epoch 822 / 1000 done..\n",
            "Training Loss 0.00012943614274263382\n",
            "Epoch 823 / 1000 done..\n",
            "Training Loss 0.00013030818081460893\n",
            "Epoch 824 / 1000 done..\n",
            "Training Loss 0.0001295848487643525\n",
            "Epoch 825 / 1000 done..\n",
            "Training Loss 0.00013069252599962056\n",
            "Epoch 826 / 1000 done..\n",
            "Training Loss 0.00012984171917196363\n",
            "Epoch 827 / 1000 done..\n",
            "Training Loss 0.00013037343160249293\n",
            "Epoch 828 / 1000 done..\n",
            "Training Loss 0.0001292539236601442\n",
            "Epoch 829 / 1000 done..\n",
            "Training Loss 0.0001295644324272871\n",
            "Epoch 830 / 1000 done..\n",
            "Training Loss 0.00012875643733423203\n",
            "Epoch 831 / 1000 done..\n",
            "Training Loss 0.00012915437400806695\n",
            "Epoch 832 / 1000 done..\n",
            "Training Loss 0.00012858762056566775\n",
            "Epoch 833 / 1000 done..\n",
            "Training Loss 0.00012907608470413834\n",
            "Epoch 834 / 1000 done..\n",
            "Training Loss 0.0001285825710510835\n",
            "Epoch 835 / 1000 done..\n",
            "Training Loss 0.00012914082617498934\n",
            "Epoch 836 / 1000 done..\n",
            "Training Loss 0.0001286100596189499\n",
            "Epoch 837 / 1000 done..\n",
            "Training Loss 0.0001292283704970032\n",
            "Epoch 838 / 1000 done..\n",
            "Training Loss 0.00012860761489719152\n",
            "Epoch 839 / 1000 done..\n",
            "Training Loss 0.00012927190982736647\n",
            "Epoch 840 / 1000 done..\n",
            "Training Loss 0.00012855669774580747\n",
            "Epoch 841 / 1000 done..\n",
            "Training Loss 0.0001292247325181961\n",
            "Epoch 842 / 1000 done..\n",
            "Training Loss 0.00012845569290220737\n",
            "Epoch 843 / 1000 done..\n",
            "Training Loss 0.00012906511256005615\n",
            "Epoch 844 / 1000 done..\n",
            "Training Loss 0.0001283211458940059\n",
            "Epoch 845 / 1000 done..\n",
            "Training Loss 0.0001288488565478474\n",
            "Epoch 846 / 1000 done..\n",
            "Training Loss 0.00012820627307519317\n",
            "Epoch 847 / 1000 done..\n",
            "Training Loss 0.00012865455937571824\n",
            "Epoch 848 / 1000 done..\n",
            "Training Loss 0.00012816840899176896\n",
            "Epoch 849 / 1000 done..\n",
            "Training Loss 0.00012856064131483436\n",
            "Epoch 850 / 1000 done..\n",
            "Training Loss 0.00012834226072300225\n",
            "Epoch 851 / 1000 done..\n",
            "Training Loss 0.0001288189523620531\n",
            "Epoch 852 / 1000 done..\n",
            "Training Loss 0.00012879367568530142\n",
            "Epoch 853 / 1000 done..\n",
            "Training Loss 0.00012888644414488226\n",
            "Epoch 854 / 1000 done..\n",
            "Training Loss 0.0001282959565287456\n",
            "Epoch 855 / 1000 done..\n",
            "Training Loss 0.00012822350254282355\n",
            "Epoch 856 / 1000 done..\n",
            "Training Loss 0.0001276748371310532\n",
            "Epoch 857 / 1000 done..\n",
            "Training Loss 0.00012784346472471952\n",
            "Epoch 858 / 1000 done..\n",
            "Training Loss 0.00012742661056108773\n",
            "Epoch 859 / 1000 done..\n",
            "Training Loss 0.00012775443610735238\n",
            "Epoch 860 / 1000 done..\n",
            "Training Loss 0.00012737706128973514\n",
            "Epoch 861 / 1000 done..\n",
            "Training Loss 0.00012775318464264274\n",
            "Epoch 862 / 1000 done..\n",
            "Training Loss 0.0001273361558560282\n",
            "Epoch 863 / 1000 done..\n",
            "Training Loss 0.00012766629515681416\n",
            "Epoch 864 / 1000 done..\n",
            "Training Loss 0.00012720690574496984\n",
            "Epoch 865 / 1000 done..\n",
            "Training Loss 0.0001274530659429729\n",
            "Epoch 866 / 1000 done..\n",
            "Training Loss 0.00012702669482678175\n",
            "Epoch 867 / 1000 done..\n",
            "Training Loss 0.00012721860548481345\n",
            "Epoch 868 / 1000 done..\n",
            "Training Loss 0.00012689756113104522\n",
            "Epoch 869 / 1000 done..\n",
            "Training Loss 0.00012705742847174406\n",
            "Epoch 870 / 1000 done..\n",
            "Training Loss 0.00012688669085036963\n",
            "Epoch 871 / 1000 done..\n",
            "Training Loss 0.00012703829270321876\n",
            "Epoch 872 / 1000 done..\n",
            "Training Loss 0.00012706573761533946\n",
            "Epoch 873 / 1000 done..\n",
            "Training Loss 0.00012722710380330682\n",
            "Epoch 874 / 1000 done..\n",
            "Training Loss 0.00012732183677144349\n",
            "Epoch 875 / 1000 done..\n",
            "Training Loss 0.00012731453170999885\n",
            "Epoch 876 / 1000 done..\n",
            "Training Loss 0.00012709572911262512\n",
            "Epoch 877 / 1000 done..\n",
            "Training Loss 0.00012688548304140568\n",
            "Epoch 878 / 1000 done..\n",
            "Training Loss 0.00012638114276342094\n",
            "Epoch 879 / 1000 done..\n",
            "Training Loss 0.00012622677604667842\n",
            "Epoch 880 / 1000 done..\n",
            "Training Loss 0.0001256754039786756\n",
            "Epoch 881 / 1000 done..\n",
            "Training Loss 0.00012573838466778398\n",
            "Epoch 882 / 1000 done..\n",
            "Training Loss 0.0001252695801667869\n",
            "Epoch 883 / 1000 done..\n",
            "Training Loss 0.0001255841343663633\n",
            "Epoch 884 / 1000 done..\n",
            "Training Loss 0.00012516921560745686\n",
            "Epoch 885 / 1000 done..\n",
            "Training Loss 0.0001256524701602757\n",
            "Epoch 886 / 1000 done..\n",
            "Training Loss 0.00012511739623732865\n",
            "Epoch 887 / 1000 done..\n",
            "Training Loss 0.0001254862145287916\n",
            "Epoch 888 / 1000 done..\n",
            "Training Loss 0.00012481003068387508\n",
            "Epoch 889 / 1000 done..\n",
            "Training Loss 0.00012504355981945992\n",
            "Epoch 890 / 1000 done..\n",
            "Training Loss 0.00012446829350665212\n",
            "Epoch 891 / 1000 done..\n",
            "Training Loss 0.00012465904001146555\n",
            "Epoch 892 / 1000 done..\n",
            "Training Loss 0.00012429944763425738\n",
            "Epoch 893 / 1000 done..\n",
            "Training Loss 0.00012448725465219468\n",
            "Epoch 894 / 1000 done..\n",
            "Training Loss 0.00012433122901711613\n",
            "Epoch 895 / 1000 done..\n",
            "Training Loss 0.00012448843335732818\n",
            "Epoch 896 / 1000 done..\n",
            "Training Loss 0.00012435954704415053\n",
            "Epoch 897 / 1000 done..\n",
            "Training Loss 0.00012436218094080687\n",
            "Epoch 898 / 1000 done..\n",
            "Training Loss 0.00012400370906107128\n",
            "Epoch 899 / 1000 done..\n",
            "Training Loss 0.00012380542466416955\n",
            "Epoch 900 / 1000 done..\n",
            "Training Loss 0.00012319456436671317\n",
            "Epoch 901 / 1000 done..\n",
            "Training Loss 0.00012292883184272796\n",
            "Epoch 902 / 1000 done..\n",
            "Training Loss 0.00012229061394464225\n",
            "Epoch 903 / 1000 done..\n",
            "Training Loss 0.00012206517567392439\n",
            "Epoch 904 / 1000 done..\n",
            "Training Loss 0.0001216793098137714\n",
            "Epoch 905 / 1000 done..\n",
            "Training Loss 0.00012152957788202912\n",
            "Epoch 906 / 1000 done..\n",
            "Training Loss 0.00012148993846494704\n",
            "Epoch 907 / 1000 done..\n",
            "Training Loss 0.00012109585804864764\n",
            "Epoch 908 / 1000 done..\n",
            "Training Loss 0.00012095061538275331\n",
            "Epoch 909 / 1000 done..\n",
            "Training Loss 0.0001203675419674255\n",
            "Epoch 910 / 1000 done..\n",
            "Training Loss 0.00012051127851009369\n",
            "Epoch 911 / 1000 done..\n",
            "Training Loss 0.00012014091771561652\n",
            "Epoch 912 / 1000 done..\n",
            "Training Loss 0.00012001475261058658\n",
            "Epoch 913 / 1000 done..\n",
            "Training Loss 0.00011925919534405693\n",
            "Epoch 914 / 1000 done..\n",
            "Training Loss 0.00011881903628818691\n",
            "Epoch 915 / 1000 done..\n",
            "Training Loss 0.00011840909428428859\n",
            "Epoch 916 / 1000 done..\n",
            "Training Loss 0.00011795301543315873\n",
            "Epoch 917 / 1000 done..\n",
            "Training Loss 0.00011773945880122483\n",
            "Epoch 918 / 1000 done..\n",
            "Training Loss 0.00011724659270839766\n",
            "Epoch 919 / 1000 done..\n",
            "Training Loss 0.00011707210796885192\n",
            "Epoch 920 / 1000 done..\n",
            "Training Loss 0.00011654985428322107\n",
            "Epoch 921 / 1000 done..\n",
            "Training Loss 0.00011638271098490804\n",
            "Epoch 922 / 1000 done..\n",
            "Training Loss 0.00011587074550334364\n",
            "Epoch 923 / 1000 done..\n",
            "Training Loss 0.00011575060489121825\n",
            "Epoch 924 / 1000 done..\n",
            "Training Loss 0.00011532181815709919\n",
            "Epoch 925 / 1000 done..\n",
            "Training Loss 0.00011525105219334364\n",
            "Epoch 926 / 1000 done..\n",
            "Training Loss 0.00011481775436550379\n",
            "Epoch 927 / 1000 done..\n",
            "Training Loss 0.00011455299681983888\n",
            "Epoch 928 / 1000 done..\n",
            "Training Loss 0.00011418575013522059\n",
            "Epoch 929 / 1000 done..\n",
            "Training Loss 0.0001139047381002456\n",
            "Epoch 930 / 1000 done..\n",
            "Training Loss 0.00011375230678822845\n",
            "Epoch 931 / 1000 done..\n",
            "Training Loss 0.00011338249896652997\n",
            "Epoch 932 / 1000 done..\n",
            "Training Loss 0.00011323946819175035\n",
            "Epoch 933 / 1000 done..\n",
            "Training Loss 0.00011273208656348288\n",
            "Epoch 934 / 1000 done..\n",
            "Training Loss 0.00011252463445998728\n",
            "Epoch 935 / 1000 done..\n",
            "Training Loss 0.0001119850276154466\n",
            "Epoch 936 / 1000 done..\n",
            "Training Loss 0.00011171910591656342\n",
            "Epoch 937 / 1000 done..\n",
            "Training Loss 0.00011123809963464737\n",
            "Epoch 938 / 1000 done..\n",
            "Training Loss 0.00011102545977337286\n",
            "Epoch 939 / 1000 done..\n",
            "Training Loss 0.00011061689292546362\n",
            "Epoch 940 / 1000 done..\n",
            "Training Loss 0.00011061207624152303\n",
            "Epoch 941 / 1000 done..\n",
            "Training Loss 0.00011020830424968153\n",
            "Epoch 942 / 1000 done..\n",
            "Training Loss 0.00011053022171836346\n",
            "Epoch 943 / 1000 done..\n",
            "Training Loss 0.00011001236998708919\n",
            "Epoch 944 / 1000 done..\n",
            "Training Loss 0.00011033655027858913\n",
            "Epoch 945 / 1000 done..\n",
            "Training Loss 0.00010950103751383722\n",
            "Epoch 946 / 1000 done..\n",
            "Training Loss 0.00010953491437248886\n",
            "Epoch 947 / 1000 done..\n",
            "Training Loss 0.00010875420412048697\n",
            "Epoch 948 / 1000 done..\n",
            "Training Loss 0.00010879903129534796\n",
            "Epoch 949 / 1000 done..\n",
            "Training Loss 0.00010824549826793373\n",
            "Epoch 950 / 1000 done..\n",
            "Training Loss 0.00010832502448465675\n",
            "Epoch 951 / 1000 done..\n",
            "Training Loss 0.00010783675679704174\n",
            "Epoch 952 / 1000 done..\n",
            "Training Loss 0.00010793138790177181\n",
            "Epoch 953 / 1000 done..\n",
            "Training Loss 0.0001074860047083348\n",
            "Epoch 954 / 1000 done..\n",
            "Training Loss 0.00010758727148640901\n",
            "Epoch 955 / 1000 done..\n",
            "Training Loss 0.00010719489364419132\n",
            "Epoch 956 / 1000 done..\n",
            "Training Loss 0.0001072181185008958\n",
            "Epoch 957 / 1000 done..\n",
            "Training Loss 0.00010686884343158454\n",
            "Epoch 958 / 1000 done..\n",
            "Training Loss 0.00010671604832168669\n",
            "Epoch 959 / 1000 done..\n",
            "Training Loss 0.00010648846364347264\n",
            "Epoch 960 / 1000 done..\n",
            "Training Loss 0.00010623186244629323\n",
            "Epoch 961 / 1000 done..\n",
            "Training Loss 0.00010617446969263256\n",
            "Epoch 962 / 1000 done..\n",
            "Training Loss 0.00010585772542981431\n",
            "Epoch 963 / 1000 done..\n",
            "Training Loss 0.00010563879186520353\n",
            "Epoch 964 / 1000 done..\n",
            "Training Loss 0.00010511134314583614\n",
            "Epoch 965 / 1000 done..\n",
            "Training Loss 0.00010480856144567952\n",
            "Epoch 966 / 1000 done..\n",
            "Training Loss 0.00010446087981108576\n",
            "Epoch 967 / 1000 done..\n",
            "Training Loss 0.00010427828237880021\n",
            "Epoch 968 / 1000 done..\n",
            "Training Loss 0.00010413993004476652\n",
            "Epoch 969 / 1000 done..\n",
            "Training Loss 0.00010401278268545866\n",
            "Epoch 970 / 1000 done..\n",
            "Training Loss 0.00010401409235782921\n",
            "Epoch 971 / 1000 done..\n",
            "Training Loss 0.00010382714390289038\n",
            "Epoch 972 / 1000 done..\n",
            "Training Loss 0.00010395243589300662\n",
            "Epoch 973 / 1000 done..\n",
            "Training Loss 0.00010362907778471708\n",
            "Epoch 974 / 1000 done..\n",
            "Training Loss 0.00010400937026133761\n",
            "Epoch 975 / 1000 done..\n",
            "Training Loss 0.00010356083657825366\n",
            "Epoch 976 / 1000 done..\n",
            "Training Loss 0.0001041132491081953\n",
            "Epoch 977 / 1000 done..\n",
            "Training Loss 0.00010327062045689672\n",
            "Epoch 978 / 1000 done..\n",
            "Training Loss 0.00010353124525863677\n",
            "Epoch 979 / 1000 done..\n",
            "Training Loss 0.00010253906657453626\n",
            "Epoch 980 / 1000 done..\n",
            "Training Loss 0.00010272633517161012\n",
            "Epoch 981 / 1000 done..\n",
            "Training Loss 0.00010194028436671942\n",
            "Epoch 982 / 1000 done..\n",
            "Training Loss 0.00010209334141109139\n",
            "Epoch 983 / 1000 done..\n",
            "Training Loss 0.00010139174992218614\n",
            "Epoch 984 / 1000 done..\n",
            "Training Loss 0.00010144279804080725\n",
            "Epoch 985 / 1000 done..\n",
            "Training Loss 0.00010082466178573668\n",
            "Epoch 986 / 1000 done..\n",
            "Training Loss 0.00010093008313560858\n",
            "Epoch 987 / 1000 done..\n",
            "Training Loss 0.00010047282557934523\n",
            "Epoch 988 / 1000 done..\n",
            "Training Loss 0.00010073626617668197\n",
            "Epoch 989 / 1000 done..\n",
            "Training Loss 0.00010040695633506402\n",
            "Epoch 990 / 1000 done..\n",
            "Training Loss 0.00010069375275634229\n",
            "Epoch 991 / 1000 done..\n",
            "Training Loss 0.00010030467819888145\n",
            "Epoch 992 / 1000 done..\n",
            "Training Loss 0.00010034040315076709\n",
            "Epoch 993 / 1000 done..\n",
            "Training Loss 9.991902334149927e-05\n",
            "Epoch 994 / 1000 done..\n",
            "Training Loss 9.986960503738374e-05\n",
            "Epoch 995 / 1000 done..\n",
            "Training Loss 9.94827423710376e-05\n",
            "Epoch 996 / 1000 done..\n",
            "Training Loss 9.929561201715842e-05\n",
            "Epoch 997 / 1000 done..\n",
            "Training Loss 9.890370711218566e-05\n",
            "Epoch 998 / 1000 done..\n",
            "Training Loss 9.881251025944948e-05\n",
            "Epoch 999 / 1000 done..\n",
            "Training Loss 9.856536780716851e-05\n",
            "tensor(70.6307)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_59U6E0AhVgK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a02ac2fa-8cd6-409d-e604-be9e082adb61"
      },
      "source": [
        "plt.plot(sc.inverse_transform(model(x_test.to(device)).cpu().data.numpy().squeeze(0)))\n",
        "plt.plot(sc.inverse_transform(y_test.data.numpy()))\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}